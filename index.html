<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT">










<meta name="description" content="做个有梦想的咸鱼">
<meta property="og:type" content="website">
<meta property="og:title" content="greenHard的个人主页">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="greenHard的个人主页">
<meta property="og:description" content="做个有梦想的咸鱼">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="greenHard的个人主页">
<meta name="twitter:description" content="做个有梦想的咸鱼">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>greenHard的个人主页</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">greenHard的个人主页</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/07/Kafka系列/10.Kafka反序列化-再均衡-拦截器/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/07/Kafka系列/10.Kafka反序列化-再均衡-拦截器/" itemprop="url">Kafka反序列化-再均衡-拦截器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-07T18:24:00+08:00">
                2019-05-07
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/07/Kafka系列/10.Kafka反序列化-再均衡-拦截器/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/07/Kafka系列/10.Kafka反序列化-再均衡-拦截器/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/07/Kafka系列/10.Kafka反序列化-再均衡-拦截器/" class="leancloud_visitors" data-flag-title="Kafka反序列化-再均衡-拦截器">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="反序列化"><a href="#反序列化" class="headerlink" title="反序列化"></a>反序列化</h2><p>Kafka内部存储的数据都是二进制数组的形式，当我们往Kafka broker中生产数据时，我们需要序列化数据二进制数组。同理，当我们消费数据的时候，我们需要将二进制数组转化成我们需要的数据，这个时候就需要反序列化。</p>
<p>Kafka 所提供的反序列化器有 <code>ByteBufferDeserializer、ByteArrayDeserializer、BytesDeserializer、DoubleDeserializer、FloatDeserializer、IntegerDeserializer、LongDeserializer、ShortDeserializer、StringDeserializer</code>，它们分别用于 <code>ByteBuffer、ByteArray、Bytes、Double、Float、Integer、Long、Short</code>及 String 类型的反序列化，这些序列化器也都实现了 <code>Deserializer</code> 接口。</p>
<h3 id="StringDeserializer"><a href="#StringDeserializer" class="headerlink" title="StringDeserializer"></a>StringDeserializer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  String encoding defaults to UTF8 and can be customized by setting the property key.deserializer.encoding,</span></span><br><span class="line"><span class="comment"> *  value.deserializer.encoding or deserializer.encoding. The first two take precedence over the last.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringDeserializer</span> <span class="keyword">implements</span> <span class="title">Deserializer</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String encoding = <span class="string">"UTF8"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">        String propertyName = isKey ? <span class="string">"key.deserializer.encoding"</span> : <span class="string">"value.deserializer.encoding"</span>;</span><br><span class="line">        Object encodingValue = configs.get(propertyName);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue == <span class="keyword">null</span>)</span><br><span class="line">            encodingValue = configs.get(<span class="string">"deserializer.encoding"</span>);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue <span class="keyword">instanceof</span> String)</span><br><span class="line">            encoding = (String) encodingValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> String(data, encoding);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Error when deserializing byte[] to string due to unsupported encoding "</span> + encoding);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// nothing to do</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="自定义Deserializer"><a href="#自定义Deserializer" class="headerlink" title="自定义Deserializer"></a>自定义Deserializer</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><h5 id="Message"><a href="#Message" class="headerlink" title="Message"></a>Message</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.consumer.derializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Message</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Message</span><span class="params">(<span class="keyword">int</span> id, String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Message&#123;"</span> +</span><br><span class="line">                <span class="string">"id="</span> + id +</span><br><span class="line">                <span class="string">", name='"</span> + name + <span class="string">'\''</span> +</span><br><span class="line">                <span class="string">'&#125;'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="MessageDerializer"><a href="#MessageDerializer" class="headerlink" title="MessageDerializer"></a>MessageDerializer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.consumer.derializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.errors.SerializationException;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Deserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义消息序列化器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MessageDeserializer</span> <span class="keyword">implements</span> <span class="title">Deserializer</span>&lt;<span class="title">Message</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Message <span class="title">deserialize</span><span class="params">(String topic, <span class="keyword">byte</span>[] data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">if</span> (data.length &lt; <span class="number">8</span>)</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"The Message data bytes length should not be less than 8."</span>);</span><br><span class="line">        <span class="comment">// 读取数据,先读取id,在读取length</span></span><br><span class="line">        ByteBuffer buffer = ByteBuffer.wrap(data);</span><br><span class="line">        <span class="comment">// 先读取id</span></span><br><span class="line">        buffer.getInt();</span><br><span class="line">        <span class="keyword">int</span> id = buffer.getInt();</span><br><span class="line">        <span class="comment">// 读取name长度</span></span><br><span class="line">        <span class="keyword">int</span> nameLength = buffer.getInt();</span><br><span class="line">        <span class="keyword">byte</span>[] nameBytes = <span class="keyword">new</span> <span class="keyword">byte</span>[nameLength];</span><br><span class="line">        buffer.get(nameBytes);</span><br><span class="line">        String name = <span class="keyword">new</span> String(nameBytes);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> Message(id, name);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.consumer.derializer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicBoolean;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 反序列化器Demo</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyDeserializerConsumerDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(MyDeserializerConsumerDemo.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> AtomicBoolean isRunning = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        KafkaConsumer&lt;String, Message&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"send-demo-topic"</span>));</span><br><span class="line">        <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, Message&gt; records = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">            records.forEach(r -&gt; LOGGER.info(<span class="string">"message is &#123;&#125;"</span>, r));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">loadProp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, MessageDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"test_group10"</span>);</span><br><span class="line">        props.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"consumer-deserializer-client"</span>);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="再均衡"><a href="#再均衡" class="headerlink" title="再均衡"></a>再均衡</h2><p><strong>再均衡是指分区的所属权从一个消费者转移到另一消费者的行为</strong>，它为消费组具备高可用性和伸缩性提供保障，使我们可以既方便又安全地删除消费组内的消费者或往消费组内添加消费者。不过在再均衡发生期间，<strong>消费组内的消费者是无法读取消息的</strong>。也就是说，在再均衡发生期间的这一小段时间内，消费组会变得不可用。</p>
<p>另外，当一个分区被重新分配给另一个消费者时，消费者当前的状态也会丢失。比如消费者消费完某个分区中的一部分消息时还没有来得及提交消费位移就发生了再均衡操作，之后这个分区又被分配给了消费组内的另一个消费者，原来被消费完的那部分消息又被重新消费一遍，也就是发生了重复消费。一般情况下，应尽量避免不必要的再均衡的发生。</p>
<h3 id="ConsumerRebalanceListener"><a href="#ConsumerRebalanceListener" class="headerlink" title="ConsumerRebalanceListener"></a>ConsumerRebalanceListener</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ConsumerRebalanceListener</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 这个方法会在再均衡开始之前和消费者停止读取消息之后被调用。</span></span><br><span class="line"><span class="comment">     * 可以通过这个回调方法来处理消费位移的提交，以此来避免一些不必要的重复消费现象的发生。</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@params</span> partitions 表示再均衡前所分配到的分区。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 这个方法会在重新分配分区之后和消费者开始读取消费之前被调用。</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@params</span> partitions 表示再均衡后所分配到的分区</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Consumer-1"><a href="#Consumer-1" class="headerlink" title="Consumer"></a>Consumer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.consumer.rebalance;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicBoolean;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 再均衡消费者</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RebalanceConsumerCommit</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> AtomicBoolean isRunning = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">        Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"test-consumer-commit-topic"</span>), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">                consumer.commitSync(currentOffsets);</span><br><span class="line">                currentOffsets.clear();</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">                <span class="comment">//do nothing.</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">                <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                    <span class="comment">// process the record.</span></span><br><span class="line">                    currentOffsets.put(</span><br><span class="line">                            <span class="keyword">new</span> TopicPartition(record.topic(), record.partition()),</span><br><span class="line">                            <span class="keyword">new</span> OffsetAndMetadata(record.offset() + <span class="number">1</span>));</span><br><span class="line">                &#125;</span><br><span class="line">                consumer.commitAsync(currentOffsets, <span class="keyword">null</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            consumer.close();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">loadProp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"test_group12"</span>);</span><br><span class="line">        props.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"rebalance-consumer-client"</span>);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>将消费位移暂存到一个局部变量 <code>currentOffsets</code> 中，这样在正常消费的时候可以通过 <code>commitAsync()</code> 方法来异步提交消费位移，在发生再均衡动作之前可以通过再均衡监听器的 <code>onPartitionsRevoked()</code> 回调执行 <code>commitSync()</code> 方法同步提交消费位移，以尽量避免一些不必要的重复消费。</p>
<h3 id="配合外部存储"><a href="#配合外部存储" class="headerlink" title="配合外部存储"></a>配合外部存储</h3><p>再均衡监听器还可以配合外部存储使用。我们将消费位移保存在数据库中，这里可以通过再均衡监听器查找分配到的分区的消费位移，并且配合 seek() 方法来进一步优化代码逻辑。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Arrays.asList(topic), <span class="keyword">new</span> ConsumerRebalanceListener() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//store offset in DB （storeOffsetToDB）</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span>(TopicPartition tp: partitions)&#123;</span><br><span class="line">            consumer.seek(tp, getOffsetFromDB(tp));<span class="comment">//从DB中读取消费位移</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>
<h2 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h2><p>之前看到了生产者拦截器的使用，对应的消费者也有相应的拦截器的概念。消费者拦截器主要在消费到消息或在提交消费位移时进行一些定制化的操作。</p>
<h3 id="ConsumerInterceptor"><a href="#ConsumerInterceptor" class="headerlink" title="ConsumerInterceptor"></a>ConsumerInterceptor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">onConsume</span><span class="params">(ConsumerRecords&lt;K, V&gt; records)</span>；</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCommit</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</span>；</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure>
<p><code>KafkaConsumer</code> 会在 poll() 方法返回之前调用拦截器的 onConsume() 方法来对消息进行相应的定制化操作，比如修改返回的消息内容、按照某种规则过滤消息（可能会减少 poll() 方法返回的消息的个数）。如果 onConsume() 方法中抛出异常，那么会被捕获并记录到日志中，但是异常不会再向上传递。</p>
<p><code>KafkaConsumer</code>会在提交完消费位移之后调用拦截器的 onCommit() 方法，可以使用这个方法来记录跟踪所提交的位移信息，比如当消费者使用 commitSync 的无参方法时，我们不知道提交的消费位移的具体细节，而使用拦截器的 onCommit() 方法却可以做到这一点。</p>
<h3 id="自定义ConsumerInterceptor"><a href="#自定义ConsumerInterceptor" class="headerlink" title="自定义ConsumerInterceptor"></a>自定义ConsumerInterceptor</h3><p>在某些业务场景中会对消息设置一个有效期的属性，如果某条消息在既定的时间窗口内无法到达，那么就会被视为无效，它也就不需要再被继续处理了。下面使用消费者拦截器来实现一个简单的消息 TTL（Time to Live，即过期时间）的功能。</p>
<h4 id="数据准备-1"><a href="#数据准备-1" class="headerlink" title="数据准备"></a>数据准备</h4><h5 id="MyConsumerInterceptor"><a href="#MyConsumerInterceptor" class="headerlink" title="MyConsumerInterceptor"></a>MyConsumerInterceptor</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.consumer.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerInterceptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.OffsetAndMetadata;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.HashMap;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者拦截器Demo</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyConsumerInterceptor</span> <span class="keyword">implements</span> <span class="title">ConsumerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(MyConsumerInterceptor.class);</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 过期时间10秒</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> <span class="keyword">long</span> EXPIRE_INTERVAL = <span class="number">10</span> * <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ConsumerRecords&lt;String, String&gt; <span class="title">onConsume</span><span class="params">(ConsumerRecords&lt;String, String&gt; records)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> now = System.currentTimeMillis();</span><br><span class="line">        Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;String, String&gt;&gt;&gt; newRecords = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        records.partitions().forEach(tp -&gt; &#123;</span><br><span class="line">            List&lt;ConsumerRecord&lt;String, String&gt;&gt; topicRecords = records.records(tp);</span><br><span class="line">            topicRecords.forEach(record -&gt; &#123;</span><br><span class="line">                List&lt;ConsumerRecord&lt;String, String&gt;&gt; newTpRecords = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">                <span class="comment">// 比较记录的时间和当前时间,如果超过指定的时间就舍弃</span></span><br><span class="line">                <span class="keyword">if</span> (now - record.timestamp() &lt; EXPIRE_INTERVAL) &#123;</span><br><span class="line">                    newTpRecords.add(record);</span><br><span class="line">                &#125;</span><br><span class="line">                <span class="keyword">if</span> (!newTpRecords.isEmpty()) &#123;</span><br><span class="line">                    newRecords.put(tp, newTpRecords);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ConsumerRecords&lt;&gt;(newRecords);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCommit</span><span class="params">(Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets)</span> </span>&#123;</span><br><span class="line">        offsets.forEach((p, o) -&gt; &#123;</span><br><span class="line">            LOG.info(<span class="string">"partition is &#123;&#125; ,offset is &#123;&#125;"</span>, p.partition(), o.offset());</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="Consumer-2"><a href="#Consumer-2" class="headerlink" title="Consumer"></a>Consumer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.consumer.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhang.consumer.derializer.Message;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicBoolean;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerInterceptorDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(ConsumerInterceptorDemo.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> AtomicBoolean isRunning = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        KafkaConsumer&lt;String, Message&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"test-consumer-commit-topic"</span>));</span><br><span class="line">        <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, Message&gt; records = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">            records.forEach(r -&gt; LOGGER.info(<span class="string">"offset  &#123;&#125; ,value &#123;&#125;"</span>, r.offset(), r.value()));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">loadProp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"test_group20"</span>);</span><br><span class="line">        props.put(ConsumerConfig.INTERCEPTOR_CLASSES_CONFIG, MyConsumerInterceptor.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"consumer-interceptor-client"</span>);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在消费者中也有拦截链的概念，和生产者的拦截链一样，也是按照 interceptor.classes 参数配置的拦截器的顺序来一一执行的（配置的时候，各个拦截器之间使用逗号隔开）。同样也要提防“副作用”的发生。如果在拦截链中某个拦截器执行失败，那么下一个拦截器会接着从上一个执行成功的拦截器继续执行。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/05/Kafka系列/9.Kafka位移提交/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/05/Kafka系列/9.Kafka位移提交/" itemprop="url">Kafka位移提交</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-05T19:25:00+08:00">
                2019-05-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/05/Kafka系列/9.Kafka位移提交/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/05/Kafka系列/9.Kafka位移提交/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/05/Kafka系列/9.Kafka位移提交/" class="leancloud_visitors" data-flag-title="Kafka位移提交">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>对于 Kafka 中的分区而言，它的每条消息都有唯一的 offset，用来表示消息在分区中对应的位置。对于消费者而言，它也有一个 offset 的概念，消费者使用 offset 来表示消费到分区中某个消息所在的位置。</p>
<p>单词“offset”可以翻译为“偏移量”，也可以翻译为“位移”，读者可能并没有过多地在意这一点：在很多中文资料中都会交叉使用“偏移量”和“位移”这两个词，并没有很严谨地进行区分。笔者对 offset 做了一些区分：对于消息在分区中的位置，我们将 offset 称为“偏移量”；对于消费者消费到的位置，将 offset 称为“位移”，有时候也会更明确地称之为“消费位移”。</p>
<p>做这一区分的目的是让读者在遇到 offset 的时候可以很容易甄别出是在讲分区存储层面的内容，还是在讲消费层面的内容，如此也可以使“偏移量”和“位移”这两个中文词汇具备更加丰富的意义。当然，对于一条消息而言，它的偏移量和消费者消费它时的消费位移是相等的，在某些不需要具体划分的场景下也可以用“消息位置”或直接用“offset”这个单词来进行表述。</p>
<p>在每次调用 poll() 方法时，它返回的是还没有被消费过的消息集（当然这个前提是消息已经存储在 Kafka 中了，并且暂不考虑异常情况的发生），要做到这一点，就需要记录上一次消费时的消费位移。并且这个消费位移必须做持久化保存，而不是单单保存在内存中，否则消费者重启之后就无法知晓之前的消费位移。再考虑一种情况，当有新的消费者加入时，那么必然会有再均衡的动作，对于同一分区而言，它可能在再均衡动作之后分配给新的消费者，如果不持久化保存消费位移，那么这个新的消费者也无法知晓之前的消费位移。</p>
<p>在旧消费者客户端中，消费位移是存储在 ZooKeeper 中的。而在新消费者客户端中，消费位移存储在 Kafka 内部的主题__consumer_offsets 中。这里把将消费位移存储起来（持久化）的动作称为“提交”，消费者在消费完消息之后需要执行消费位移的提交。</p>
<p><img src="/2019/05/05/Kafka系列/9.Kafka位移提交/1.png" alt></p>
<p>参考上图中的消费位移，x表示某一次拉取操作中此分区消息的最大偏移量，假设当前消费者已经消费了x位置的消息，那么我们就可以说消费者的消费位移为x，图中也用了 lastConsumedOffset 这个单词来标识它。</p>
<p>不过需要非常明确的是，当前消费者需要提交的消费位移并不是x，而是x+1，对应于上图中的 position，它表示下一条需要拉取的消息的位置。读者可能看过一些相关资料，里面所讲述的内容可能是提交的消费位移就是当前所消费到的消费位移，即提交的是x，这明显是错误的。类似的错误还体现在对 LEO（Log End Offset） 的解读上。在消费者中还有一个 committed offset 的概念，它表示已经提交过的消费位移。</p>
<p>KafkaConsumer 类提供了 position(TopicPartition) 和 committed(TopicPartition) 两个方法来分别获取上面所说的 position 和 committed offset 的值。这两个方法的定义如下所示。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">long</span> <span class="title">position</span><span class="params">(TopicPartition partition)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> OffsetAndMetadata <span class="title">committed</span><span class="params">(TopicPartition partition)</span></span></span><br></pre></td></tr></table></figure>
<p>为了论证 lastConsumedOffset、committed offset 和 position 之间的关系，我们使用上面的这两个方法来做相关演示。我们向某个主题中分区编号为0的分区发送若干消息，之后再创建一个消费者去消费其中的消息，等待消费完这些消息之后就同步提交消费位移，最后我们观察一下 lastConsumedOffset、committed offset 和 position 的值。示例代码如下。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.TopicPartition;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumerPositionDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(KafkaConsumerPositionDemo.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">        TopicPartition tp = <span class="keyword">new</span> TopicPartition(<span class="string">"test_consumer2_topic"</span>, <span class="number">0</span>);</span><br><span class="line">        consumer.assign(Arrays.asList(tp));</span><br><span class="line">        <span class="comment">// 当前消费到的位移</span></span><br><span class="line">        <span class="keyword">long</span> lastConsumedOffset = -<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">            <span class="keyword">if</span> (records.isEmpty()) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords = records.records(tp);</span><br><span class="line">            lastConsumedOffset = partitionRecords.get(partitionRecords.size() - <span class="number">1</span>).offset();</span><br><span class="line">            <span class="comment">// 同步提交消费位移</span></span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">        LOG.info(<span class="string">"consumed offset is &#123;&#125;"</span>, lastConsumedOffset);</span><br><span class="line">        OffsetAndMetadata offsetAndMetadata = consumer.committed(tp);</span><br><span class="line">        LOG.info(<span class="string">"consumed offset is &#123;&#125;"</span>, offsetAndMetadata.offset());</span><br><span class="line">        <span class="keyword">long</span> position = consumer.position(tp);</span><br><span class="line">        LOG.info(<span class="string">"the offset of the next record is &#123;&#125;"</span>, position);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">loadProp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"test_group4"</span>);</span><br><span class="line">        props.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"consumer-position-client"</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,<span class="string">"earliest"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>最终的输出结果如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumed offset is <span class="number">21</span></span><br><span class="line">consumed offset is <span class="number">22</span></span><br><span class="line">the offset of the next record is <span class="number">22</span></span><br></pre></td></tr></table></figure>
<p>可以看出，消费者消费到此分区消息的最大偏移量为21，对应的消费位移 lastConsumedOffset 也就是21。在消费完之后就执行同步提交，但是最终结果显示所提交的位移 committed offset 为22，并且下一次所要拉取的消息的起始偏移量 position 也为22。在本示例中，position = committed offset = lastConsumedOffset + 1，当然 position 和 committed offset 并不会一直相同，这一点会在下面的示例中有所体现。</p>
<p><img src="/2019/05/05/Kafka系列/9.Kafka位移提交/2.png" alt="图3-7"></p>
<p>对于位移提交的具体时机的把握也很有讲究，有可能会造成重复消费和消息丢失的现象。参考上图，当前一次 poll() 操作所拉取的消息集为 [x+2, x+7]，x+2 代表上一次提交的消费位移，说明已经完成了 x+1 之前（包括 x+1 在内）的所有消息的消费，x+5 表示当前正在处理的位置。如果拉取到消息之后就进行了位移提交，即提交了 x+8，那么当前消费 x+5 的时候遇到了异常，在故障恢复之后，我们重新拉取的消息是从 x+8 开始的。也就是说，x+5 至 x+7 之间的消息并未能被消费，如此便发生了消息丢失的现象。</p>
<p>再考虑另外一种情形，位移提交的动作是在消费完所有拉取到的消息之后才执行的，那么当消费 x+5 的时候遇到了异常，在故障恢复之后，我们重新拉取的消息是从 x+2 开始的。也就是说，x+2 至 x+4 之间的消息又重新消费了一遍，故而又发生了重复消费的现象。</p>
<p>而实际情况还会有比这两种更加复杂的情形，比如第一次的位移提交的位置为 x+8，而下一次的位移提交的位置为 x+4，后面会做进一步的分析。</p>
<p>在 Kafka 中默认的消费位移的提交方式是自动提交，这个由消费者客户端参数 enable.auto.commit 配置，默认值为 true。当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数 auto.commit.interval.ms 配置，默认值为5秒，此参数生效的前提是 enable.auto.commit 参数为 true。</p>
<p>在默认的方式下，消费者每隔5秒会将拉取到的每个分区中最大的消息位移进行提交。自动位移提交的动作是在 poll() 方法的逻辑里完成的，在每次真正向服务端发起拉取请求之前会检查是否可以进行位移提交，如果可以，那么就会提交上一次轮询的位移。</p>
<p>在 Kafka 消费的编程逻辑中位移提交是一大难点，自动提交消费位移的方式非常简便，它免去了复杂的位移提交逻辑，让编码更简洁。但随之而来的是重复消费和消息丢失的问题。假设刚刚提交完一次消费位移，然后拉取一批消息进行消费，在下一次自动提交消费位移之前，消费者崩溃了，那么又得从上一次位移提交的地方重新开始消费，这样便发生了重复消费的现象（对于再均衡的情况同样适用）。我们可以通过减小位移提交的时间间隔来减小重复消息的窗口大小，但这样并不能避免重复消费的发送，而且也会使位移提交更加频繁。</p>
<p><img src="/2019/05/05/Kafka系列/9.Kafka位移提交/3.png" alt="图3-8"></p>
<p>按照一般思维逻辑而言，自动提交是延时提交，重复消费可以理解，那么消息丢失又是在什么情形下会发生的呢？我们来看一下上图中的情形。拉取线程A不断地拉取消息并存入本地缓存，比如在 BlockingQueue 中，另一个处理线程B从缓存中读取消息并进行相应的逻辑处理。假设目前进行到了第 y+1 次拉取，以及第m次位移提交的时候，也就是 x+6 之前的位移已经确认提交了，处理线程B却还正在消费 x+3 的消息。此时如果处理线程B发生了异常，待其恢复之后会从第m此位移提交处，也就是 x+6 的位置开始拉取消息，那么 x+3 至 x+6 之间的消息就没有得到相应的处理，这样便发生消息丢失的现象。</p>
<p>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免，与此同时，自动位移提交也无法做到精确的位移管理。在 Kafka 中还提供了手动位移提交的方式，这样可以使得开发人员对消费位移的管理控制更加灵活。很多时候并不是说拉取到消息就算消费完成，而是需要将消息写入数据库、写入本地缓存，或者是更加复杂的业务处理。在这些场景下，所有的业务处理完成才能认为消息被成功消费，手动的提交方式可以让开发人员根据程序的逻辑在合适的地方进行位移提交。开启手动提交功能的前提是消费者客户端参数 enable.auto.commit 配置为 false，示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br></pre></td></tr></table></figure>
<h2 id="手动提交"><a href="#手动提交" class="headerlink" title="手动提交"></a>手动提交</h2><p>手动提交可以细分为同步提交和异步提交，对应于 <code>KafkaConsumer</code> 中的 <code>commitSync()</code> 和 <code>commitAsync()</code> 两种类型的方法。</p>
<h3 id="同步提交"><a href="#同步提交" class="headerlink" title="同步提交"></a>同步提交</h3><h4 id="不带参数的同步提交"><a href="#不带参数的同步提交" class="headerlink" title="不带参数的同步提交"></a>不带参数的同步提交</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 同步提交</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerSyncCommit</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(KafkaConsumerPositionDemo.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"test-consumer-commit-topic"</span>));</span><br><span class="line">        <span class="keyword">for</span> (; ; ) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            records.forEach(record -&gt; &#123;</span><br><span class="line">                <span class="comment">//biz handler.</span></span><br><span class="line">                LOG.info(<span class="string">"offset:&#123;&#125;"</span>, record.offset());</span><br><span class="line">                LOG.info(<span class="string">"value:&#123;&#125;"</span>, record.value());</span><br><span class="line">                LOG.info(<span class="string">"key:&#123;&#125;"</span>, record.key());</span><br><span class="line">            &#125;);</span><br><span class="line">            <span class="comment">// 同步提交</span></span><br><span class="line">            consumer.commitSync();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">loadProp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"test_group4"</span>);</span><br><span class="line">        props.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"consumer-commit-client"</span>);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到示例中先对拉取到的每一条消息做相应的逻辑处理，然后对整个消息集做同步提交。针对上面的示例还可以修改为批量处理+批量提交的方式，核心代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">    consumer.subscribe(Collections.singletonList(<span class="string">"test-consumer-commit-topic"</span>));</span><br><span class="line">    AtomicInteger count = <span class="keyword">new</span> AtomicInteger();</span><br><span class="line">    <span class="keyword">for</span> (; ; ) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">        records.forEach(record -&gt; &#123;</span><br><span class="line">            <span class="comment">//biz handler.</span></span><br><span class="line">            LOG.info(<span class="string">"offset:&#123;&#125;"</span>, record.offset());</span><br><span class="line">            LOG.info(<span class="string">"value:&#123;&#125;"</span>, record.value());</span><br><span class="line">            LOG.info(<span class="string">"key:&#123;&#125;"</span>, record.key());</span><br><span class="line">            <span class="keyword">if</span> (count.incrementAndGet() == <span class="number">10</span>) &#123;</span><br><span class="line">                consumer.commitSync();</span><br><span class="line">                count.set(<span class="number">0</span>);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面的示例使用一个count统计,也就是示例中等于10的时候，再做相应的批量处理，之后再做批量提交。这两种示例都有重复消费的问题，如果在业务逻辑处理完之后，并且在同步位移提交前，程序出现了崩溃，那么待恢复之后又只能从上一次位移提交的地方拉取消息，由此在两次位移提交的窗口中出现了重复消费的现象。</p>
<blockquote>
<p>commitSync() 方法会根据 poll() 方法拉取的最新位移来进行提交（注意提交的值对应于本节第1张图中 position 的位置），只要没有发生不可恢复的错误（Unrecoverable Error），它就会阻塞消费者线程直至位移提交完成。对于不可恢复的错误，比如 CommitFailedException、WakeupException、InterruptException、AuthenticationException、AuthorizationException 等，我们可以将其捕获并做针对性的处理。</p>
</blockquote>
<h4 id="带参数的同步提交"><a href="#带参数的同步提交" class="headerlink" title="带参数的同步提交"></a>带参数的同步提交</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">    consumer.subscribe(Collections.singletonList(<span class="string">"test-consumer-commit-topic"</span>));</span><br><span class="line">    Map&lt;TopicPartition, OffsetAndMetadata&gt; offset = <span class="keyword">new</span> HashMap&lt;&gt;(<span class="number">3</span>);</span><br><span class="line">    TopicPartition tp = <span class="keyword">new</span> TopicPartition(<span class="string">"test-consumer-commit-topic"</span>, <span class="number">1</span>);</span><br><span class="line">    OffsetAndMetadata om = <span class="keyword">new</span> OffsetAndMetadata(<span class="number">2</span>, <span class="string">"no meta data"</span>);</span><br><span class="line">    offset.put(tp, om);</span><br><span class="line">    <span class="keyword">for</span> (; ; ) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">        records.forEach(record -&gt; &#123;</span><br><span class="line">            <span class="comment">//biz handler.</span></span><br><span class="line">            LOG.info(<span class="string">"Partition:&#123;&#125;,offset:&#123;&#125;"</span>, record.partition(), record.offset());</span><br><span class="line">        &#125;);</span><br><span class="line">        <span class="comment">// 同步提交</span></span><br><span class="line">        consumer.commitSync(offset);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在实际应用中，很少会有这种每消费一条消息就提交一次消费位移的必要场景。commitSync() 方法本身是同步执行的，会耗费一定的性能，而示例中的这种提交方式会将性能拉到一个相当低的点。我们分区的粒度划分提交位移的界限，这里我们就要用到了第10节中提及的 ConsumerRecords 类的 partitions() 方法和 records(TopicPartition) 方法，关键示例代码如代码清单如下。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">1000</span>);</span><br><span class="line">        <span class="keyword">for</span> (TopicPartition partition : records.partitions()) &#123;</span><br><span class="line">            List&lt;ConsumerRecord&lt;String, String&gt;&gt; partitionRecords =</span><br><span class="line">                    records.records(partition);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : partitionRecords) &#123;</span><br><span class="line">                <span class="comment">//do some logical processing.</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">long</span> lastConsumedOffset = partitionRecords</span><br><span class="line">                    .get(partitionRecords.size() - <span class="number">1</span>).offset();</span><br><span class="line">            consumer.commitSync(Collections.singletonMap(partition,</span><br><span class="line">                    <span class="keyword">new</span> OffsetAndMetadata(lastConsumedOffset + <span class="number">1</span>)));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="异步提交"><a href="#异步提交" class="headerlink" title="异步提交"></a>异步提交</h3><p>与 comitSync() 方法相反，异步提交的方式（commitAsync()）在执行的时候消费者线程不会被阻塞，可能在提交消费位移的结果还未返回之前就开始了新一次的拉取操作。异步提交可以使消费者的性能得到一定的增强。commitAsync 方法有三个不同的重载方法，具体定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(OffsetCommitCallback callback)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitAsync</span><span class="params">(<span class="keyword">final</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; offsets,</span></span></span><br><span class="line"><span class="function"><span class="params">            OffsetCommitCallback callback)</span></span></span><br></pre></td></tr></table></figure>
<p>第一个无参的方法和第三个方法中的 offsets 都很好理解，对照 commitSync() 方法即可。关键的是这里的第二个方法和第三个方法中的 callback 参数，它提供了一个异步提交的回调方法，当位移提交完成后会回调 OffsetCommitCallback 中的 onComplete() 方法。关键代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 异步提交</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerAsyncCommit</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(ConsumerAsyncCommit.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"test-consumer-commit-topic"</span>));</span><br><span class="line">        <span class="keyword">for</span> (; ; ) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">            records.forEach(record -&gt; &#123;</span><br><span class="line">                <span class="comment">//biz handler.</span></span><br><span class="line">                LOG.info(<span class="string">"offset:&#123;&#125;,value:&#123;&#125;,key:&#123;&#125;"</span>, record.offset(), record.value(), record.key());</span><br><span class="line">                consumer.commitAsync((offsets, exception) -&gt; &#123;</span><br><span class="line">                    <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;</span><br><span class="line">                        exception.printStackTrace();</span><br><span class="line">                    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                        offsets.forEach((p, data) -&gt; &#123;</span><br><span class="line">                            LOG.info(<span class="string">"commit success partition:&#123;&#125;,offset:&#123;&#125;"</span>, p.partition(), data.offset());</span><br><span class="line">                        &#125;);</span><br><span class="line"></span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">loadProp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"test_group7"</span>);</span><br><span class="line">        props.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"consumer-commit-async-client"</span>);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>commitAsync() 提交的时候同样会有失败的情况发生，那么我们应该怎么处理呢？读者有可能想到的是重试，问题的关键也就在这里了。如果某一次异步提交的消费位移为x，但是提交失败了，然后下一次又异步提交了消费位移为x+y，这次成功了。如果这里引入了重试机制，前一次的异步提交的消费位移在重试的时候提交成功了，那么此时的消费位移又变为了x。如果此时发生异常（或者再均衡），那么恢复之后的消费者（或者新的消费者）就会从x处开始消费消息，这样就发生了重复消费的问题。</p>
<p>为此我们可以设置一个递增的序号来维护异步提交的顺序，每次位移提交之后就增加序号相对应的值。在遇到位移提交失败需要重试的时候，可以检查所提交的位移和序号的值的大小，如果前者小于后者，则说明有更大的位移已经提交了，不需要再进行本次重试；如果两者相同，则说明可以进行重试提交。除非程序编码错误，否则不会出现前者大于后者的情况。</p>
<h3 id="同步和异步提交"><a href="#同步和异步提交" class="headerlink" title="同步和异步提交"></a>同步和异步提交</h3><p><strong>关键代码如下</strong>:</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">        <span class="comment">//poll records and do some logical processing.</span></span><br><span class="line">        consumer.commitAsync();</span><br><span class="line">    &#125;</span><br><span class="line">&#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        consumer.commitSync();</span><br><span class="line">    &#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">        consumer.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果位移提交失败的情况经常发生，那么说明系统肯定出现了故障，在一般情况下，位移提交失败的情况很少发生，不重试也没有关系，后面的提交也会有成功的。重试会增加代码逻辑的复杂度，不重试会增加重复消费的概率。如果消费者异常退出，那么这个重复消费的问题就很难避免，因为这种情况下无法及时提交消费位移；如果消费者正常退出或发生再均衡的情况，那么可以在退出或再均衡执行之前使用同步提交的方式做最后的把关。</p>
<h2 id="控制或优雅的关闭消费"><a href="#控制或优雅的关闭消费" class="headerlink" title="控制或优雅的关闭消费"></a>控制或优雅的关闭消费</h2><h3 id="控制消费"><a href="#控制消费" class="headerlink" title="控制消费"></a>控制消费</h3><p><code>KafkaConsumer</code> 提供了对消费速度进行控制的方法，在有些应用场景下我们可能需要暂停某些分区的消费而先消费其他分区，当达到一定条件时再恢复这些分区的消费。<code>KafkaConsumer</code> 中使用 pause() 和 resume() 方法来分别实现暂停某些分区在拉取操作时返回数据给客户端和恢复某些分区向客户端返回数据的操作。这两个方法的具体定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">pause</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">resume</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br></pre></td></tr></table></figure>
<p><code>KafkaConsumer</code> 还提供了一个无参的 paused() 方法来返回被暂停的分区集合，此方法的具体定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> Set&lt;TopicPartition&gt; <span class="title">paused</span><span class="params">()</span></span></span><br></pre></td></tr></table></figure>
<h3 id="优雅的关闭消费"><a href="#优雅的关闭消费" class="headerlink" title="优雅的关闭消费"></a>优雅的关闭消费</h3><p>之前的示例展示的都是使用一个 while 循环来包裹住 poll() 方法及相应的消费逻辑，如何优雅地退出这个循环也很有考究。细心的读者可能注意到有些示例代码并不是以 while(true) 的形式做简单的包裹，而是使用 while(isRunning.get()) 的方式，这样可以通过在其他地方设定 isRunning.set(false) 来退出 while 循环。还有一种方式是调用 <code>KafkaConsumer</code> 的 wakeup() 方法，wakeup() 方法是 <code>KafkaConsumer</code> 中唯一可以从其他线程里安全调用的方法（<code>KafkaConsumer</code> 是非线程安全的），调用 wakeup() 方法后可以退出 poll() 的逻辑，并抛出 <code>WakeupException</code> 的异常，我们也不需要处理 <code>WakeupException</code>的异常，它只是一种跳出循环的方式。</p>
<p>跳出循环以后一定要显式地执行关闭动作以释放运行过程中占用的各种系统资源，包括内存资源、Socket 连接等。<code>KafkaConsumer</code> 提供了 close() 方法来实现关闭，close() 方法有三种重载方法，分别如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(Duration timeout)</span></span></span><br><span class="line"><span class="function">@Deprecated</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">(<span class="keyword">long</span> timeout, TimeUnit timeUnit)</span></span></span><br></pre></td></tr></table></figure>
<p>第二种方法是通过 timeout 参数来设定关闭方法的最长执行时间，有些内部的关闭逻辑会耗费一定的时间，比如设置了自动提交消费位移，这里还会做一次位移提交的动作；而第一种方法没有 timeout 参数，这并不意味着会无限制地等待，它内部设定了最长等待时间（30秒）；第三种方法已被标记为 @Deprecated，可以不考虑。 一个相对完整的消费程序的逻辑可以参考下面的关键代码：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 优雅的关闭消费者</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">GentlestCloseConsumer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(GentlestCloseConsumer.class);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> AtomicBoolean isRunning = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">true</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> AtomicInteger count = <span class="keyword">new</span> AtomicInteger();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"test-consumer-commit-topic"</span>));</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofSeconds(<span class="number">10</span>));</span><br><span class="line">                records.forEach(record -&gt; &#123;</span><br><span class="line">                    <span class="comment">//biz handler.</span></span><br><span class="line">                    LOG.info(<span class="string">"offset:&#123;&#125;,value:&#123;&#125;,key:&#123;&#125;"</span>, record.offset(), record.value(), record.key());</span><br><span class="line">                    consumer.commitAsync();</span><br><span class="line">                    <span class="comment">// 消费10条之后优雅的关闭</span></span><br><span class="line">                    <span class="keyword">if</span> (count.incrementAndGet() == <span class="number">10</span>) &#123;</span><br><span class="line">                        isRunning.set(<span class="keyword">false</span>);</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (WakeupException e) &#123;</span><br><span class="line">            <span class="comment">// ignore the error</span></span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            <span class="comment">// do some logic process.</span></span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                consumer.commitSync();</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">                consumer.close();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">     <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">loadProp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"test_group9"</span>);</span><br><span class="line">        props.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"gentlest-consumer-client"</span>);</span><br><span class="line">        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="keyword">false</span>);</span><br><span class="line">        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">"earliest"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/03/Kafka系列/8.Kafka消息消费/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/03/Kafka系列/8.Kafka消息消费/" itemprop="url">Kafka消息消费</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-03T21:57:00+08:00">
                2019-05-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/03/Kafka系列/8.Kafka消息消费/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/03/Kafka系列/8.Kafka消息消费/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/03/Kafka系列/8.Kafka消息消费/" class="leancloud_visitors" data-flag-title="Kafka消息消费">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="正常消费"><a href="#正常消费" class="headerlink" title="正常消费"></a>正常消费</h2><p>Kafka 中的消费是基于拉模式的。消息的消费一般有两种模式：推模式和拉模式。推模式是服务端主动将消息推送给消费者，而拉模式是消费者主动向服务端发起请求来拉取消息。</p>
<blockquote>
<p>Kafka性能非常好，如果采用推的模式，很可能把消费者冲垮了。</p>
</blockquote>
<h3 id="poll"><a href="#poll" class="headerlink" title="poll"></a>poll</h3><p>对于 poll() 方法而言，如果某些分区中没有可供消费的消息，那么此分区对应的消息拉取的结果就为空；如果订阅的所有分区中都没有可供消费的消息，那么 poll() 方法返回为空的消息集合。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> Duration timeout)</span></span></span><br><span class="line"><span class="function">@Deprecated</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ConsumerRecords&lt;K, V&gt; <span class="title">poll</span><span class="params">(<span class="keyword">final</span> <span class="keyword">long</span> timeout)</span></span></span><br></pre></td></tr></table></figure>
<p>poll(long) 方法中 timeout 的时间单位固定为毫秒，而 poll(Duration) 方法可以根据 Duration 中的 ofMillis()、ofSeconds()、ofMinutes()、ofHours() 等多种不同的方法指定不同的时间单位，灵活性更强。并且 poll(long) 方法也已经被标注为 @Deprecated，虽然目前还可以使用，如果条件允许的话，还是推荐使用 poll(Duration) 的方式。</p>
<blockquote>
<p>timeout 的设置取决于应用程序对响应速度的要求，比如需要在多长时间内将控制权移交给执行轮询的应用线程。可以直接将 timeout 设置为0，这样 poll() 方法会立刻返回，而不管是否已经拉取到了消息。如果应用线程唯一的工作就是从 Kafka 中拉取并消费消息，则可以将这个参数设置为最大值 Long.MAX_VALUE。</p>
</blockquote>
<h3 id="ConsumerRecord"><a href="#ConsumerRecord" class="headerlink" title="ConsumerRecord"></a>ConsumerRecord</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> offset;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">long</span> timestamp;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> TimestampType timestampType;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> serializedKeySize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> serializedValueSize;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Headers headers;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> Long checksum;</span><br><span class="line">	    <span class="comment">//省略若干方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>topic 和 partition 这两个字段分别代表消息所属主题的名称和所在分区的编号。</p>
</li>
<li><p>offset 表示消息在所属分区的偏移量。</p>
</li>
<li><p>timestamp 表示时间戳，与此对应的 timestampType 表示时间戳的类型。timestampType 有两种类型：CreateTime和LogAppendTime，分别代表消息创建的时间戳和消息追加到日志的时间戳。headers 表示消息的头部内容。</p>
</li>
<li><p>key 和 value 分别表示消息的键和消息的值，一般业务应用要读取的就是 value。</p>
</li>
<li>serializedKeySize 和 serializedValueSize 分别表示 key 和 value 经过序列化之后的大小，如果 key 为空，则 serializedKeySize 值为-1。同样，如果 value 为空，则 serializedValueSize 的值也会为-1。</li>
<li>checksum 是 CRC32 的校验值。</li>
</ul>
<h3 id="ConsumerRecords"><a href="#ConsumerRecords" class="headerlink" title="ConsumerRecords"></a>ConsumerRecords</h3><p>poll() 方法的返回值类型是 <code>ConsumerRecords</code>，它用来表示一次拉取操作所获得的消息集，内部包含了若干 <code>ConsumerRecord</code>，它提供了一个 iterator() 方法来循环遍历消息集内部的消息，iterator() 方法的定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterator&lt;ConsumerRecord&lt;K, V&gt;&gt; iterator()</span><br></pre></td></tr></table></figure>
<p>除此之外，我们还可以按照分区维度来进行消费，这一点很有用，在手动提交位移时尤为明显。<code>ConsumerRecords</code> 类提供了一个 records(TopicPartition) 方法来获取消息集中指定分区的消息，此方法的定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> List&lt;ConsumerRecord&lt;K, V&gt;&gt; records(TopicPartition partition)</span><br></pre></td></tr></table></figure>
<p>主要的示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line"><span class="keyword">for</span> (TopicPartition tp : records.partitions()) &#123;</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records.records(tp)) &#123;</span><br><span class="line">        System.out.println(record.partition()+<span class="string">" : "</span>+record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上面示例中的 <code>ConsumerRecords.partitions()</code> 方法用来获取消息集中所有分区。在 <code>ConsumerRecords</code> 类中还提供了按照主题维度来进行消费的方法，这个方法是 records(TopicPartition) 的重载方法，具体定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> Iterable&lt;ConsumerRecord&lt;K, V&gt;&gt; records(String topic)</span><br></pre></td></tr></table></figure>
<p><code>ConsumerRecords</code>类中并没提供与 partitions() 类似的 topics() 方法来查看拉取的消息集中所包含的主题列表，如果要按照主题维度来进行消费，那么只能根据消费者订阅主题时的列表来进行逻辑处理了。下面的示例演示了如何使用 <code>ConsumerRecords</code> 中的 record(String topic) 方法：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">List&lt;String&gt; topicList = Arrays.asList(topic1, topic2);</span><br><span class="line">consumer.subscribe(topicList);</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">        <span class="keyword">for</span> (String topic : topicList) &#123;</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records.records(topic)) &#123;</span><br><span class="line">                System.out.println(record.topic() + <span class="string">" : "</span> + record.value());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;<span class="keyword">finally</span> &#123;</span><br><span class="line">    consumer.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在 <code>ConsumerRecords</code> 类中还提供了几个方法来方便开发人员对消息集进行处理：</p>
<ul>
<li><p>count() 方法用来计算出消息集中的消息个数，返回类型是 int。</p>
</li>
<li><p>isEmpty() 方法用来判断消息集是否为空，返回类型是 boolean。</p>
</li>
<li><p>empty() 方法用来获取一个空的消息集，返回类型是 <code>ConsumerRecords&lt;K，V&gt;</code>。</p>
</li>
</ul>
<blockquote>
<p>到目前为止，可以简单地认为 poll() 方法只是拉取一下消息而已，但就其内部逻辑而言并不简单，它涉及消费位移、消费者协调器、组协调器、消费者的选举、分区分配的分发、再均衡的逻辑、心跳等内容，在后面的章节中会循序渐进地介绍这些内容。</p>
</blockquote>
<h2 id="指定位移消费"><a href="#指定位移消费" class="headerlink" title="指定位移消费"></a>指定位移消费</h2><p>试想一下，有这样一个场景。当一个新的消费组建立的时候，它根本没有可以查找的消费位移。或者消费组内的一个新消费者订阅了一个新的主题，它也没有可以查找的消费位移。当 __consumer_offsets 主题中有关这个消费组的位移信息过期而被删除后，它也没有可以查找的消费位移。</p>
<p><img src="/2019/05/03/Kafka系列/8.Kafka消息消费/1.png" alt></p>
<p><code>auto.offset.reset</code> 参数除了<code>earliest</code>和<code>latest</code>还有一个可配置的值—<code>none</code>，配置为此值就意味着出现查到不到消费位移的时候，既不从最新的消息位置处开始消费，也不从最早的消息位置处开始消费，此时会报出 <code>NoOffsetForPartitionException</code> 异常，示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.clients.consumer.NoOffsetForPartitionException: Undefined offset with no reset policy <span class="keyword">for</span> partitions: [topic-demo-<span class="number">3</span>, topic-demo-<span class="number">0</span>, topic-demo-<span class="number">2</span>, topic-demo-<span class="number">1</span>].</span><br></pre></td></tr></table></figure>
<p>到目前为止，我们知道消息的拉取是根据 poll() 方法中的逻辑来处理的，这个 poll() 方法中的逻辑对于普通的开发人员而言是一个黑盒，无法精确地掌控其消费的起始位置。提供的 <code>auto.offset.reset</code> 参数也只能在找不到消费位移或位移越界的情况下粗粒度地从开头或末尾开始消费。有些时候，我们需要一种更细粒度的掌控，可以让我们从特定的位移处开始拉取消息，而 <code>KafkaConsumer</code>中的 seek() 方法正好提供了这个功能，让我们得以追前消费或回溯消费。seek() 方法的具体定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seek</span><span class="params">(TopicPartition partition, <span class="keyword">long</span> offset)</span></span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>seek() 方法中的参数 partition 表示分区，而 offset 参数用来指定从分区的哪个位置开始消费。seek() 方法只能重置消费者分配到的分区的消费位置，<strong>而分区的分配是在 poll() 方法的调用过程中实现的</strong>。也就是说，在执行 seek() 方法之前需要先执行一次 poll() 方法，等到分配到分区之后才可以重置消费位置。</p>
</blockquote>
<p><strong>部分代码如下:</strong></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">    consumer.subscribe(Collections.singletonList(<span class="string">"test-consumer-commit-topic"</span>));</span><br><span class="line">    Set&lt;TopicPartition&gt; assignment;</span><br><span class="line">    <span class="comment">// 1. 保证可以拿到分区</span></span><br><span class="line">    <span class="keyword">do</span> &#123;</span><br><span class="line">        consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">        assignment = consumer.assignment();</span><br><span class="line">    &#125; <span class="keyword">while</span> (assignment.size() == <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 2. 从每个分区的第5个位置开始消费</span></span><br><span class="line">    <span class="keyword">for</span> (TopicPartition tp : assignment) &#123;</span><br><span class="line">        consumer.seek(tp, <span class="number">5</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 3. 消费记录</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">        records.forEach(record -&gt; &#123;</span><br><span class="line">            <span class="comment">//biz handler.</span></span><br><span class="line">            LOG.info(<span class="string">"offset:&#123;&#125;,value:&#123;&#125;,key:&#123;&#125;"</span>, record.offset(), record.value(), record.key());</span><br><span class="line">            consumer.commitAsync();</span><br><span class="line">        &#125;);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果对未分配到的分区执行 seek() 方法，那么会报出 <code>IllegalStateException</code> 的异常。类似在调用 subscribe() 方法之后直接调用 seek() 方法。</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** 其他的一些方法 */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekToBeginning</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">seekToEnd</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;TopicPartition, Long&gt; <span class="title">beginningOffsets</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;TopicPartition, Long&gt; <span class="title">beginningOffsets</span><span class="params">(Collection&lt;TopicPartition&gt; partitions,</span></span></span><br><span class="line"><span class="function"><span class="params">            									  Duration timeout)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;TopicPartition, Long&gt; <span class="title">endOffsets</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;TopicPartition, Long&gt; <span class="title">endOffsets</span><span class="params">(Collection&lt;TopicPartition&gt;partitions,</span></span></span><br><span class="line"><span class="function"><span class="params">                                            Duration timeout)</span>  </span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;TopicPartition, OffsetAndTimestamp&gt; <span class="title">offsetsForTimes</span><span class="params">(Map&lt;TopicPartition, </span></span></span><br><span class="line"><span class="function"><span class="params">                                                               Long timestampsToSearch)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> Map&lt;TopicPartition, OffsetAndTimestamp&gt; <span class="title">offsetsForTimes</span><span class="params">(Map&lt;TopicPartition, </span></span></span><br><span class="line"><span class="function"><span class="params">											Long timestampsToSearch, Duration timeout)</span></span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/03/Kafka系列/7.Kafka消费者开发/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/03/Kafka系列/7.Kafka消费者开发/" itemprop="url">Kafka消费者客户端开发</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-03T19:38:00+08:00">
                2019-05-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/03/Kafka系列/7.Kafka消费者开发/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/03/Kafka系列/7.Kafka消费者开发/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/03/Kafka系列/7.Kafka消费者开发/" class="leancloud_visitors" data-flag-title="Kafka消费者客户端开发">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>在了解了消费者与消费组之间的概念之后，我们就可以着手进行消费者客户端的开发了。在 Kafka 的历史中，消费者客户端同生产者客户端一样也经历了两个大版本：第一个是于 Kafka 开源之初使用 Scala 语言编写的客户端，我们可以称之为旧消费者客户端（Old Consumer）或 Scala 消费者客户端；第二个是从 Kafka 0.9.x 版本开始推出的使用 Java 编写的客户端，我们可以称之为新消费者客户端（New Consumer）或 Java 消费者客户端，它弥补了旧客户端中存在的诸多设计缺陷。</p>
<p>本节主要介绍目前流行的新消费者（Java 语言编写的）客户端，而旧消费者客户端已被淘汰，故不再做相应的介绍了。</p>
<h2 id="客户端开发"><a href="#客户端开发" class="headerlink" title="客户端开发"></a>客户端开发</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerRecords;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.KafkaConsumer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringDeserializer;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.time.Duration;</span><br><span class="line"><span class="keyword">import</span> java.util.Collections;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.atomic.AtomicBoolean;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 基础Consumer</span></span><br><span class="line"><span class="comment"> * 1. 配置消费者客户端参数及创建相应的消费者实例。</span></span><br><span class="line"><span class="comment"> * 2. 订阅主题。</span></span><br><span class="line"><span class="comment"> * 3. 拉取消息并消费。</span></span><br><span class="line"><span class="comment"> * 4. 提交消费位移。</span></span><br><span class="line"><span class="comment"> * 5. 关闭消费者实例。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SimpleConsumerDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">final</span> Logger LOG = LoggerFactory.getLogger(SimpleConsumerDemo.class);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> AtomicBoolean isRunning = <span class="keyword">new</span> AtomicBoolean(<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 1. 配置消费者客户端参数及创建相应的消费者实例。</span></span><br><span class="line">        KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(loadProp());</span><br><span class="line">        <span class="comment">// 2. 订阅主题</span></span><br><span class="line">        consumer.subscribe(Collections.singletonList(<span class="string">"test_consumer1_topic"</span>));</span><br><span class="line">        <span class="comment">// 3. 拉取消息并消费。</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">while</span> (isRunning.get()) &#123;</span><br><span class="line">                ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">100</span>));</span><br><span class="line">                records.forEach(record -&gt;</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">//biz handler.</span></span><br><span class="line">                    LOG.info(<span class="string">"offset:&#123;&#125;"</span>, record.offset());</span><br><span class="line">                    LOG.info(<span class="string">"value:&#123;&#125;"</span>, record.value());</span><br><span class="line">                    LOG.info(<span class="string">"key:&#123;&#125;"</span>, record.key());</span><br><span class="line">                &#125;);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            LOG.error(<span class="string">"出现了异常. "</span>, e);</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="comment">// 5. 关闭消费者实例。</span></span><br><span class="line">            consumer.close();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">loadProp</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());</span><br><span class="line">        props.put(ConsumerConfig.GROUP_ID_CONFIG, <span class="string">"test_group4"</span>);</span><br><span class="line">        props.put(ConsumerConfig.CLIENT_ID_CONFIG, <span class="string">"consumer-client"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="必要的参数配置"><a href="#必要的参数配置" class="headerlink" title="必要的参数配置"></a>必要的参数配置</h2><ul>
<li><p><strong>bootstrap.servers</strong></p>
<p>该参数的释义和生产者客户端 <code>KafkaProducer</code> 中的相同，用来指定连接 Kafka 集群所需的 broker 地址清单，具体内容形式为 host1:port1,host2:post，可以设置一个或多个地址，中间用逗号隔开，此参数的默认值为“”。注意这里并非需要设置集群中全部的 broker 地址，消费者会从现有的配置中查找到全部的 Kafka 集群成员。这里设置两个以上的 broker 地址信息，当其中任意一个宕机时，消费者仍然可以连接到 Kafka 集群上。</p>
</li>
<li><p><strong>group.id</strong></p>
<p>消费者隶属的消费组的名称，默认值为“”。如果设置为空，则会报出异常：Exception in thread “main” org.apache.kafka.common.errors.InvalidGroupIdException: The configured groupId is invalid。一般而言，这个参数需要设置成具有一定的业务意义的名称。</p>
</li>
<li><p><strong>key.deserializer &amp; value.deserializer</strong></p>
<p>与生产者客户端 <code>KafkaProducer</code> 中的<code>key.serializer</code>和<code>value.serializer</code> 参数对应。消费者从 broker 端获取的消息格式都是字节数组（byte[]）类型，所以需要执行相应的反序列化操作才能还原成原有的对象格式。这两个参数分别用来指定消息中 key 和 value 所需反序列化操作的反序列化器，这两个参数无默认值。注意这里必须填写反序列化器类的全限定名，例如：<code>org.apache.kafka.common.serialization.StringDeserializer</code></p>
</li>
</ul>
<h2 id="订阅主题和分区"><a href="#订阅主题和分区" class="headerlink" title="订阅主题和分区"></a>订阅主题和分区</h2><p>在创建好消费者之后，我们就需要为该消费者订阅相关的主题了。一个消费者可以订阅一个或多个主题，既可以以集合的形式订阅多个主题，也可以以正则表达式的形式订阅特定模式的主题。subscribe 的几个重载方法如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics, ConsumerRebalanceListener listener)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Collection&lt;String&gt; topics)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern, ConsumerRebalanceListener listener)</span></span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">subscribe</span><span class="params">(Pattern pattern)</span></span></span><br></pre></td></tr></table></figure>
<p>对于消费者使用集合的方式（subscribe(Collection)）来订阅主题而言，比较容易理解，订阅了什么主题就消费什么主题中的消息。如果前后两次订阅了不同的主题，那么消费者以最后一次的为准。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Arrays.asList(topic1));</span><br><span class="line">consumer.subscribe(Arrays.asList(topic2));</span><br></pre></td></tr></table></figure>
<p>上面的示例中，最终消费者订阅的是 topic2，而不是 topic1，也不是 topic1 和 topic2 的并集。</p>
<p>如果消费者采用的是正则表达式的方式（subscribe(Pattern)）订阅，在之后的过程中，如果有人又创建了新的主题，并且主题的名字与正则表达式相匹配，那么这个消费者就可以消费到新添加的主题中的消息。如果应用程序需要消费多个主题，并且可以处理不同的类型，那么这种订阅方式就很有效。在 Kafka 和其他系统之间进行数据复制时，这种正则表达式的方式就显得很常见。正则表达式的方式订阅的示例如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.subscribe(Pattern.compile(<span class="string">"topic-.*"</span>));</span><br></pre></td></tr></table></figure>
<p>subscribe 的重载方法中有一个参数类型是 ConsumerRebalance- Listener，这个是用来设置相应的再均衡监听器的。</p>
<p>消费者不仅可以通过 <code>KafkaConsumer.subscribe()</code> 方法订阅主题，还可以直接订阅某些主题的特定分区，在 <code>KafkaConsumer</code> 中还提供了一个 assign() 方法来实现这些功能，此方法的具体定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">assign</span><span class="params">(Collection&lt;TopicPartition&gt; partitions)</span></span></span><br></pre></td></tr></table></figure>
<p>这个方法只接受一个参数 partitions，用来指定需要订阅的分区集合。这里补充说明一下 TopicPartition 类，在 Kafka 的客户端中，它用来表示分区，这个类的部分内容如下所示。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">TopicPartition</span> <span class="keyword">implements</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">TopicPartition</span><span class="params">(String topic, <span class="keyword">int</span> partition)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.partition = partition;</span><br><span class="line">        <span class="keyword">this</span>.topic = topic;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> partition;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">topic</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> topic;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//省略hashCode()、equals()和toString()方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>TopicPartition</code> 类只有2个属性：topic 和 partition，分别代表分区所属的主题和自身的分区编号，这个类可以和我们通常所说的主题—分区的概念映射起来。</p>
<p>这里只订阅 topic-demo 主题中分区编号为0的分区，相关代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Arrays.asList(<span class="keyword">new</span> TopicPartition(<span class="string">"topic-demo"</span>, <span class="number">0</span>)));</span><br></pre></td></tr></table></figure>
<p>有读者会有疑问：如果我们事先并不知道主题中有多少个分区怎么办？<code>KafkaConsumer</code> 中的 partitionsFor() 方法可以用来查询指定主题的元数据信息，partitionsFor() 方法的具体定义如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;PartitionInfo&gt; <span class="title">partitionsFor</span><span class="params">(String topic)</span></span></span><br></pre></td></tr></table></figure>
<p>其中 PartitionInfo 类型即为主题的分区元数据信息，此类的主要结构如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionInfo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> partition;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node leader;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node[] replicas;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node[] inSyncReplicas;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Node[] offlineReplicas;</span><br><span class="line">	<span class="comment">//这里省略了构造函数、属性提取、toString等方法</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>PartitionInfo</code>类中的属性 topic 表示主题名称，partition 代表分区编号，leader 代表分区的 leader 副本所在的位置，replicas 代表分区的 AR 集合，inSyncReplicas 代表分区的 ISR 集合，offlineReplicas 代表分区的 OSR 集合。 通过 partitionFor() 方法的协助，我们可以通过 assign() 方法来实现订阅主题（全部分区）的功能，示例参考如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">List&lt;TopicPartition&gt; partitions = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);</span><br><span class="line"><span class="keyword">if</span> (partitionInfos != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">for</span> (PartitionInfo tpInfo : partitionInfos) &#123;</span><br><span class="line">        partitions.add(<span class="keyword">new</span> TopicPartition(tpInfo.topic(), tpInfo.partition()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">consumer.assign(partitions);</span><br></pre></td></tr></table></figure>
<p>既然有订阅，那么就有取消订阅，可以使用 <code>KafkaConsumer</code> 中的 unsubscribe() 方法来取消主题的订阅。这个方法既可以取消通过 subscribe(Collection) 方式实现的订阅，也可以取消通过 subscribe(Pattern) 方式实现的订阅，还可以取消通过 assign(Collection) 方式实现的订阅。示例代码如下：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe();</span><br></pre></td></tr></table></figure>
<p>如果将 subscribe(Collection) 或 assign(Collection) 中的集合参数设置为空集合，那么作用等同于 unsubscribe() 方法，下面示例中的三行代码的效果相同：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">consumer.unsubscribe();</span><br><span class="line">consumer.subscribe(<span class="keyword">new</span> ArrayList&lt;String&gt;());</span><br><span class="line">consumer.assign(<span class="keyword">new</span> ArrayList&lt;TopicPartition&gt;());</span><br></pre></td></tr></table></figure>
<p>如果没有订阅任何主题或分区，那么再继续执行消费程序的时候会报出 IllegalStateException 异常：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.IllegalStateException: Consumer is not subscribed to any topics or assigned any partitions</span><br></pre></td></tr></table></figure>
<p>集合订阅的方式 subscribe(Collection)、正则表达式订阅的方式 subscribe(Pattern) 和指定分区的订阅方式 assign(Collection) 分表代表了三种不同的订阅状态：AUTO_TOPICS、AUTO_PATTERN 和 USER_ASSIGNED（如果没有订阅，那么订阅状态为 NONE）。然而这三种状态是互斥的，在一个消费者中只能使用其中的一种，否则会报出 IllegalStateException 异常：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.IllegalStateException: Subscription to topics, partitions and pattern are mutually exclusive.</span><br></pre></td></tr></table></figure>
<p>通过 subscribe() 方法订阅主题具有消费者自动再均衡的功能，在多个消费者的情况下可以根据分区分配策略来自动分配各个消费者与分区的关系。当消费组内的消费者增加或减少时，分区分配关系会自动调整，以实现消费负载均衡及故障自动转移。<strong>而通过 assign() 方法订阅分区时，是不具备消费者自动均衡的功能的</strong>，其实这一点从 assign() 方法的参数中就可以看出端倪，两种类型的 subscribe() 都有 <code>ConsumerRebalanceListener</code> 类型参数的方法，而 assign() 方法却没有。</p>
<h2 id="重要的参数配置"><a href="#重要的参数配置" class="headerlink" title="重要的参数配置"></a>重要的参数配置</h2><h3 id="fetch-min-bytes"><a href="#fetch-min-bytes" class="headerlink" title="fetch.min.bytes"></a>fetch.min.bytes</h3><p>此属性允许消费者 在获取记录时指定它希望从broker接收的最小数据量。如果broker接收来自消费者的记录请求，但新记录的字节数小于<code>min.fetch.bytes</code>，则broker将等待更多消息可用，然后再将记录发送回消费者。这减少了消费者和broker的负担，因为在主题没有太多新活动（或者当天的活动时间较少）的情况下，他们必须处理较少的来回消息。如果消费者在没有太多可用数据时使用过多CPU，或者在拥有大量消费者时减少代理的负载，则需要将此参数设置为高于默认值。</p>
<h3 id="fetch-max-wait-ms"><a href="#fetch-max-wait-ms" class="headerlink" title="fetch.max.wait.ms"></a>fetch.max.wait.ms</h3><p>通过设置<code>fetch.min.bytes</code>，您可以告诉Kafka在响应消费者之前等待有足够的数据发送。 <code>fetch.max.wait.ms</code>允许您控制等待的时间。默认情况下，Kafka将等待500毫秒。如果没有足够的数据流入Kafka主题以满足返回的最小数据量，则会导致高达500毫秒的额外延迟。如果要限制潜在延迟（通常由于SLA控制应用程序的最大延迟），可以将<code>fetch.max.wait.ms</code>设置为较低的值。如果将<code>fetch.max.wait.ms</code>设置为100 ms并将<code>fetch.min.bytes</code>设置为1 MB，Kafka将接收来自使用者的提取请求，并在数据返回1 MB或100之后响应数据ms，先发生者。</p>
<h3 id="max-partition-fetch-bytes"><a href="#max-partition-fetch-bytes" class="headerlink" title="max.partition.fetch.bytes"></a>max.partition.fetch.bytes</h3><p>此属性控制服务器将为每个分区返回的最大字节数。默认值为1 MB，这意味着当KafkaConsumer.poll()返回ConsumerRecords时，记录对象最多将使用分配给使用者的每个分区的<code>max.partition.fetch.bytes</code>。因此，如果一个主题有20个分区，并且您有5个消费者，则每个消费者需要有4 MB的内存可用于消费者记录。</p>
<h3 id="session-timeout-ms"><a href="#session-timeout-ms" class="headerlink" title="session.timeout.ms"></a>session.timeout.ms</h3><ul>
<li>消费者在被认为是活着的情况下与broker脱离联系的时间默认为3秒。如果超过<code>session.timeout.ms</code>而没有消费者向组协调器发送心跳，则认为它已经死亡，组协调器将触发使用者组的重新平衡，以将死区消费者的分区分配给组中的其他消费者。</li>
<li><code>heartbeat.interval.ms</code>控制KafkaConsumer.poll()方法将心跳发送到组协调器的频率，而session.timeout.ms控制消费者可以多长时间不发送心跳。</li>
<li>因此，这两个属性通常一起修改<code>heatbeat.interval.ms</code>必须低于<code>session.timeout.ms</code>，并且通常设置为超时值的三分之一。因此，如果<code>session.timeout.ms</code>为3秒，则<code>heartbeat.interval.ms</code>应为1秒。将<code>session.timeout.ms</code>设置为低于默认值将允许使用者组更快地检测并从故障中恢复，但是由于消费者花费更长时间来完成轮询循环或垃圾回收，也可能导致不必要的重新平衡。将<code>session.timeout.ms</code>设置得更高将减少意外重新平衡的可能性，但也意味着需要更长时间才能检测到真正的故障。</li>
</ul>
<h3 id="auto-offset-reset"><a href="#auto-offset-reset" class="headerlink" title="auto.offset.reset"></a>auto.offset.reset</h3><p>此属性控制消费者在开始读取其没有提交的偏移量的分区时的行为，或者它所拥有的提交的偏移量是否无效（通常是因为消费者长时间失败以至于具有该偏移量的记录是很早之前的数据了）。默认值为“latest”，这意味着缺少有效的偏移量，消费者将从最新的记录开始读取（消费者开始运行后写入的记录）。替代方案是“earliest”，这意味着缺少有效的偏移量，消费者将从一开始就读取分区中的所有数据。</p>
<h3 id="enable-auto-commit"><a href="#enable-auto-commit" class="headerlink" title="enable.auto.commit"></a>enable.auto.commit</h3><p>此参数控制使用者是否自动提交偏移量，默认为true。如果您希望控制何时提交偏移量，则将其设置为false，这对于最小化重复并避免丢失数据是必要的。如果将<code>enable.auto.commit</code>设置为true，那么您可能还想控制使用<code>auto.commit.interval.ms</code>提交偏移的频率。</p>
<h3 id="partition-assignment-strategy"><a href="#partition-assignment-strategy" class="headerlink" title="partition.assignment.strategy"></a>partition.assignment.strategy</h3><p><code>partition.assignment.strategy</code>允许您选择分区分配策略。默认值为<code>org.apache.kafka.clients.consumer.RangeAssignor</code>，它实现上述Range策略。您可以将其替换为<code>org.apache.kafka.clients.consumer.RoundRobinAssignor</code>。更高级的选项是实现您自己的分配策略，在这种情况下，<code>partition.assignment.strategy</code>应指向您的类的名称。</p>
<p><code>PartitionAssignor</code>是一个类，在给定的消费者和主题的情况下，决定将哪些分区分配给哪个消费者。默认情况下，Kafka有两个分配策略：</p>
<ul>
<li><strong>Range</strong><br>为每个消费者分配其订阅的每个主题的连续分区子集。因此，如果消费者C1和C2订阅了两个主题T1和T2，并且每个主题都有三个分区，则C1将从主题T1和T2分配0和1分区，而C2将从这些主题分配分区2 。由于每个主题的分区数量不均匀，并且每个主题的分配都是独立完成的，因此第一个消费者最终会得到比第二个更多的分区。只要使用Range赋值且消费者数量不能巧妙地划分每个主题中的分区数，就会发生这种情况。</li>
<li><strong>RoundRobin</strong><br>从所有订阅的主题中获取所有分区，并按顺序逐个分配给消费者。如果先前描述的C1和C2使用RoundRobin赋值，则C1将具有来自主题T1的分区0和2以及来自主题T2的分区1。 C2将具有来自主题T1的分区1以及来自主题T2的分区0和2。通常，如果所有消费者都订阅了相同的主题（非常常见的情况），则RoundRobin分配最终会使所有消费者具有相同数量的分区（或最多1个分区差异）。</li>
</ul>
<h3 id="client-id"><a href="#client-id" class="headerlink" title="client.id"></a>client.id</h3><p>这可以是任何字符串，并且代理将使用它来标识从客户端发送的消息。它用于记录和指标。</p>
<h3 id="max-poll-records"><a href="#max-poll-records" class="headerlink" title="max.poll.records"></a>max.poll.records</h3><p>这将控制对poll()的单次调用将返回的最大记录数。这有助于控制应用程序在轮询循环中需要处理的数据量。</p>
<h3 id="receive-buffer-bytes-amp-send-buffer-bytes"><a href="#receive-buffer-bytes-amp-send-buffer-bytes" class="headerlink" title="receive.buffer.bytes &amp; send.buffer.bytes"></a>receive.buffer.bytes &amp; send.buffer.bytes</h3><p>这些是写入和读取数据时套接字使用的TCP发送和接收缓冲区的大小。如果这些设置为-1，则将使用操作系统默认值。当生产者或消费者与不同数据中心的代理进行通信时，增加这些内容可能是个好主意，因为这些网络链接通常具有更高的延迟和更低的带宽。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/" itemprop="url">Kafka消费者和消费者组</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-03T17:46:00+08:00">
                2019-05-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/03/Kafka系列/6.Kafka消费者和消费者组/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/" class="leancloud_visitors" data-flag-title="Kafka消费者和消费者组">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>与生产者对应的是消费者，应用程序可以通过 KafkaConsumer 来订阅主题，并从订阅的主题中拉取消息。不过在使用 KafkaConsumer 消费消息之前需要先了解消费者和消费组的概念，否则无法理解如何使KafkaConsumer。</p>
<p>消费者（Consumer）负责订阅 Kafka 中的主题（Topic），并且从订阅的主题上拉取消息。与其他一些消息中间件不同的是：在 Kafka 的消费理念中还有一层消费组（Consumer Group）的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。</p>
<p><img src="/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/1.png" alt></p>
<p>如上图所示，某个主题中共有4个分区（Partition）：P0、P1、P2、P3。有两个消费组A和B都订阅了这个主题，消费组A中有4个消费者（C0、C1、C2和C3），消费组B中有2个消费者（C4和C5）。按照 Kafka 默认的规则，最后的分配结果是消费组A中的每一个消费者分配到1个分区，消费组B中的每一个消费者分配到2个分区，两个消费组之间互不影响。每个消费者只能消费所分配到的分区中的消息。换言之，每一个分区只能被一个消费组中的一个消费者所消费。</p>
<p><img src="/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/2.png" alt></p>
<p>我们再来看一下消费组内的消费者个数变化时所对应的分区分配的演变。假设目前某消费组内只有一个消费者C0，订阅了一个主题，这个主题包含7个分区：P0、P1、P2、P3、P4、P5、P6。也就是说，这个消费者C0订阅了7个分区，具体分配情形参考上图。</p>
<p><img src="/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/3.png" alt></p>
<p>此时消费组内又加入了一个新的消费者C1，按照既定的逻辑，需要将原来消费者C0的部分分区分配给消费者C1消费，如上图所示。消费者C0和C1各自负责消费所分配到的分区，彼此之间并无逻辑上的干扰。</p>
<p><img src="/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/4.png" alt></p>
<p>紧接着消费组内又加入了一个新的消费者C2，消费者C0、C1和C2按照上图中的方式各自负责消费所分配到的分区。</p>
<p>消费者与消费组这种模型可以让整体的消费能力具备横向伸缩性，我们可以增加（或减少）消费者的个数来提高（或降低）整体的消费能力。对于分区数固定的情况，一味地增加消费者并不会让消费能力一直得到提升，如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区。参考下图，一共有8个消费者，7个分区，那么最后的消费者C7由于分配不到任何分区而无法消费任何消息。</p>
<p><img src="/2019/05/03/Kafka系列/6.Kafka消费者和消费者组/5.png" alt></p>
<p>以上分配逻辑都是基于默认的分区分配策略进行分析的，可以通过消费者客户端参数 <code>partition.assignment.strategy</code> 来设置消费者与订阅主题之间的分区分配策略。</p>
<p>对于消息中间件而言，一般有两种消息投递模式：点对点（P2P，Point-to-Point）模式和发布/订阅（Pub/Sub）模式。点对点模式是基于队列的，消息生产者发送消息到队列，消息消费者从队列中接收消息。发布订阅模式定义了如何向一个内容节点发布和订阅消息，这个内容节点称为主题（Topic），主题可以认为是消息传递的中介，消息发布者将消息发布到某个主题，而消息订阅者从主题中订阅消息。主题使得消息的订阅者和发布者互相保持独立，不需要进行接触即可保证消息的传递，发布/订阅模式在消息的一对多广播时采用。Kafka 同时支持两种消息投递模式，而这正是得益于消费者与消费组模型的契合：</p>
<ul>
<li>如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。</li>
<li>如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布/订阅模式的应用。</li>
</ul>
<p>消费组是一个逻辑上的概念，它将旗下的消费者归为一类，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数 group.id 来配置，默认值为空字符串。</p>
<p>消费者并非逻辑上的概念，它是实际的应用实例，它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。</p>
<h2 id="组和分区均衡"><a href="#组和分区均衡" class="headerlink" title="组和分区均衡"></a>组和分区均衡</h2><p>正如我们在上面看到的那样，消费者组中的消费者在他们订阅的主题中共享分区的所有权。当我们向该组添加新的使用者时，它开始使用先前由另一个使用者使用的分区的消息。消费者关闭或崩溃时也会发生同样的事情;它离开了组，它曾经消耗的分区将被剩下的一个消费者使用。当消费者组正在消费的主题被修改时（例如，如果管理员添加新分区），也会将分区重新分配给消费者</p>
<p>将分区所有权从一个消费者转移到另一个消费者称为<code>Rebalance</code>。<code>Rebalance</code>非常重要，因为它们为消费者群体提供了高可用性和可扩展性（允许我们轻松安全地添加和删除消费者），但在正常的事件过程中，它们是相当不受欢迎的。</p>
<p>在<code>Rebalance</code>期间，消费者不能消费消息，因此Rebalance基本上是整个消费者群体不可用的短暂窗口。此外，移动分区时一个消费者到另一个消费者，消费者失去了现状;再次建立自己的状态。在后面的内容中，我们将讨论如何安全地处理重新平衡，如果它正在缓存任何数据，它将需要刷新其缓存 - 减慢应用程序的速度，直到消费者如何避免不必要的数据。</p>
<p>消费者维护消费者组成员资格的方式以及分配给他们的分区的所有权是通过向指定为组协调员的<code>Kafka broker</code>发送心跳（此<code>broker</code>对于不同的消费者组可以是不同的）。只要消费者定期发送心跳，就会认为它是活着的，并且处理来自其分区的消息。当消费者轮询（即，检索记录）并且提交它已经消耗的记录时，发送心跳。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/03/Kafka系列/5.Kafka生产者客户端原理/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/03/Kafka系列/5.Kafka生产者客户端原理/" itemprop="url">生产者客户端原理解析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-03T15:16:00+08:00">
                2019-05-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/03/Kafka系列/5.Kafka生产者客户端原理/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/03/Kafka系列/5.Kafka生产者客户端原理/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/03/Kafka系列/5.Kafka生产者客户端原理/" class="leancloud_visitors" data-flag-title="生产者客户端原理解析">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="1-整体架构"><a href="#1-整体架构" class="headerlink" title="1. 整体架构"></a>1. 整体架构</h2><p>下面我们来看一下生产者客户端的整体架构，如下图所示。</p>
<p><img src="/2019/05/03/Kafka系列/5.Kafka生产者客户端原理/1.png" alt></p>
<p>整个生产者客户端由两个线程协调运行，这两个线程分别为主线程和 Sender 线程（发送线程）。在主线程中由 <code>KafkaProducer</code> 创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（<code>RecordAccumulator</code>，也称为消息收集器）中。Sender 线程负责从 <code>RecordAccumulator</code> 中获取消息并将其发送到 Kafka 中。</p>
<h3 id="1-1-KafkaProducer"><a href="#1-1-KafkaProducer" class="headerlink" title="1.1 KafkaProducer"></a>1.1 KafkaProducer</h3><h4 id="1-1-1-doSend"><a href="#1-1-1-doSend" class="headerlink" title="1.1.1 doSend"></a>1.1.1 doSend</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey,serializedValue, headers, interceptCallback, remainingWaitMs);</span><br><span class="line">    <span class="keyword">if</span> (result.batchIsFull || result.newBatchCreated) &#123;</span><br><span class="line">        log.trace(<span class="string">"Waking up the sender since topic &#123;&#125; partition &#123;&#125; is either full or getting a new batch"</span>, record.topic(), partition);</span><br><span class="line">        <span class="keyword">this</span>.sender.wakeup();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="1-2-RecordAccumulator"><a href="#1-2-RecordAccumulator" class="headerlink" title="1.2 RecordAccumulator"></a>1.2 RecordAccumulator</h3><h4 id="1-2-1-append"><a href="#1-2-1-append" class="headerlink" title="1.2.1 append"></a>1.2.1 append</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;TopicPartition, Deque&lt;ProducerBatch&gt;&gt; batches;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> RecordAppendResult <span class="title">append</span><span class="params">(TopicPartition tp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> timestamp,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">byte</span>[] key,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">byte</span>[] value,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 Header[] headers,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 Callback callback,</span></span></span><br><span class="line"><span class="function"><span class="params">                                 <span class="keyword">long</span> maxTimeToBlock)</span> <span class="keyword">throws</span> InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">// We keep track of the number of appending thread to make sure we do not miss batches in</span></span><br><span class="line">    <span class="comment">// abortIncompleteBatches().</span></span><br><span class="line">    appendsInProgress.incrementAndGet();</span><br><span class="line">    ByteBuffer buffer = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">if</span> (headers == <span class="keyword">null</span>) headers = Record.EMPTY_HEADERS;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// check if we have an in-progress batch</span></span><br><span class="line">        Deque&lt;ProducerBatch&gt; dq = getOrCreateDeque(tp);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>);</span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// we don't have an in-progress record batch try to allocate a new batch</span></span><br><span class="line">        <span class="keyword">byte</span> maxUsableMagic = apiVersions.maxUsableProduceMagic();</span><br><span class="line">        <span class="keyword">int</span> size = Math.max(<span class="keyword">this</span>.batchSize, AbstractRecords.estimateSizeInBytesUpperBound(maxUsableMagic, compression, key, value, headers));</span><br><span class="line">        log.trace(<span class="string">"Allocating a new &#123;&#125; byte message buffer for topic &#123;&#125; partition &#123;&#125;"</span>, size, tp.topic(), tp.partition());</span><br><span class="line">        buffer = free.allocate(size, maxTimeToBlock);</span><br><span class="line">        <span class="keyword">synchronized</span> (dq) &#123;</span><br><span class="line">            <span class="comment">// Need to check if producer is closed again after grabbing the dequeue lock.</span></span><br><span class="line">            <span class="keyword">if</span> (closed)</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> KafkaException(<span class="string">"Producer closed while send in progress"</span>);</span><br><span class="line"></span><br><span class="line">            RecordAppendResult appendResult = tryAppend(timestamp, key, value, headers, callback, dq);</span><br><span class="line">            <span class="keyword">if</span> (appendResult != <span class="keyword">null</span>) &#123;</span><br><span class="line">                <span class="comment">// Somebody else found us a batch, return the one we waited for! Hopefully this doesn't happen often...</span></span><br><span class="line">                <span class="keyword">return</span> appendResult;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            MemoryRecordsBuilder recordsBuilder = recordsBuilder(buffer, maxUsableMagic);</span><br><span class="line">            ProducerBatch batch = <span class="keyword">new</span> ProducerBatch(tp, recordsBuilder, time.milliseconds());</span><br><span class="line">            FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, headers, callback, time.milliseconds()));</span><br><span class="line"></span><br><span class="line">            dq.addLast(batch);</span><br><span class="line">            incomplete.add(batch);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Don't deallocate this buffer in the finally block as it's being used in the record batch</span></span><br><span class="line">            buffer = <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">new</span> RecordAppendResult(future, dq.size() &gt; <span class="number">1</span> || batch.isFull(), <span class="keyword">true</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer != <span class="keyword">null</span>)</span><br><span class="line">            free.deallocate(buffer);</span><br><span class="line">        appendsInProgress.decrementAndGet();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-2-2-getOrCreateDeque"><a href="#1-2-2-getOrCreateDeque" class="headerlink" title="1.2.2 getOrCreateDeque"></a>1.2.2 getOrCreateDeque</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Get the deque for the given topic-partition, creating it if necessary.</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> Deque&lt;ProducerBatch&gt; <span class="title">getOrCreateDeque</span><span class="params">(TopicPartition tp)</span> </span>&#123;</span><br><span class="line">    Deque&lt;ProducerBatch&gt; d = <span class="keyword">this</span>.batches.get(tp);</span><br><span class="line">    <span class="keyword">if</span> (d != <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> d;</span><br><span class="line">    d = <span class="keyword">new</span> ArrayDeque&lt;&gt;();</span><br><span class="line">    Deque&lt;ProducerBatch&gt; previous = <span class="keyword">this</span>.batches.putIfAbsent(tp, d);</span><br><span class="line">    <span class="keyword">if</span> (previous == <span class="keyword">null</span>)</span><br><span class="line">        <span class="keyword">return</span> d;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">        <span class="keyword">return</span> previous;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>RecordAccumulator</code> 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。<code>RecordAccumulator</code> 缓存的大小可以通过生产者客户端参数 <code>buffer.memory</code> 配置，默认值为 33554432B，即32MB。如果生产者发送消息的速度超过发送到服务器的速度，则会导致生产者空间不足，这个时候 <code>KafkaProducer</code> 的 send() 方法调用要么被阻塞，要么抛出异常，这个取决于参数 <code>max.block.ms</code> 的配置，此参数的默认值为60000，即60秒。</p>
<p>主线程中发送过来的消息都会被追加到 <code>RecordAccumulator</code> 的某个双端队列（Deque）中，在 <code>RecordAccumulator</code> 的内部为每个分区都维护了一个双端队列，队列中的内容就是 <code>ProducerBatch</code>，即 Deque。消息写入缓存时，追加到双端队列的尾部；Sender 读取消息时，从双端队列的头部读取。注意 <code>ProducerBatch</code>不是 <code>ProducerRecord</code>，<code>ProducerBatch</code>中可以包含一至多个 <code>ProducerRecord</code>。通俗地说，<code>ProducerRecord</code> 是生产者中创建的消息，而 <code>ProducerBatch</code> 是指一个消息批次，<code>ProducerRecord</code> 会被包含在 <code>ProducerBatch</code> 中，这样可以使字节的使用更加紧凑。与此同时，将较小的 <code>ProducerRecord</code> 拼凑成一个较大的 <code>ProducerBatch</code>，也可以减少网络请求的次数以提升整体的吞吐量。如果生产者客户端需要向很多分区发送消息，则可以将 <code>buffer.memory</code> 参数适当调大以增加整体的吞吐量。</p>
<p>消息在网络上都是以字节（Byte）的形式传输的，在发送之前需要创建一块内存区域来保存对应的消息。在 Kafka 生产者客户端中，通过<code>java.io.ByteBuffer</code> 实现消息内存的创建和释放。不过频繁的创建和释放是比较耗费资源的，在 <code>RecordAccumulator</code>的内部还有一个<code>BufferPool</code>，它主要用来实现 <code>ByteBuffer</code> 的复用，以实现缓存的高效利用。不过 <code>BufferPool</code> 只针对特定大小的 <code>ByteBuffer</code>进行管理，而其他大小的 <code>ByteBuffer</code> 不会缓存进 <code>BufferPool</code> 中，这个特定的大小由 <code>batch.size</code> 参数来指定，默认值为16384B，即16KB。我们可以适当地调大 <code>batch.size</code> 参数以便多缓存一些消息。</p>
<p><code>ProducerBatch</code> 的大小和 <code>batch.size</code> 参数也有着密切的关系。当一条消息（<code>ProducerRecord</code>）流入 <code>RecordAccumulator</code> 时，会先寻找与消息分区所对应的双端队列（如果没有则新建），再从这个双端队列的尾部获取一个 <code>ProducerBatch</code>（如果没有则新建），查看 <code>ProducerBatch</code> 中是否还可以写入这个 <code>ProducerRecord</code>，如果可以则写入，如果不可以则需要创建一个新的 <code>ProducerBatch</code>。在新建 <code>ProducerBatch</code> 时评估这条消息的大小是否超过 batch.size 参数的大小，如果不超过，那么就以 <code>batch.size</code> 参数的大小来创建 <code>ProducerBatch</code>，这样在使用完这段内存区域之后，可以通过 <code>BufferPool</code> 的管理来进行复用；如果超过，那么就以评估的大小来创建 <code>ProducerBatch</code>，这段内存区域不会被复用。</p>
<h3 id="1-3-Sender"><a href="#1-3-Sender" class="headerlink" title="1.3 Sender"></a>1.3 Sender</h3><h4 id="1-3-1-run"><a href="#1-3-1-run" class="headerlink" title="1.3.1 run"></a>1.3.1 run</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">long</span> pollTimeout = sendProducerData(now);</span><br><span class="line">    client.poll(pollTimeout, now);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-3-2-sendProducerData"><a href="#1-3-2-sendProducerData" class="headerlink" title="1.3.2 sendProducerData"></a>1.3.2 sendProducerData</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">long</span> <span class="title">sendProducerData</span><span class="params">(<span class="keyword">long</span> now)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// ... 元数据更新</span></span><br><span class="line">    </span><br><span class="line">        <span class="comment">// 移除不需要发送消息的结点</span></span><br><span class="line">        Iterator&lt;Node&gt; iter = result.readyNodes.iterator();</span><br><span class="line">        <span class="keyword">long</span> notReadyTimeout = Long.MAX_VALUE;</span><br><span class="line">        <span class="keyword">while</span> (iter.hasNext()) &#123;</span><br><span class="line">            Node node = iter.next();</span><br><span class="line">            <span class="keyword">if</span> (!<span class="keyword">this</span>.client.ready(node, now)) &#123;</span><br><span class="line">                iter.remove();</span><br><span class="line">                notReadyTimeout = Math.min(notReadyTimeout, <span class="keyword">this</span>.client.pollDelayMs(node, now));</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// create produce requests</span></span><br><span class="line">        Map&lt;Integer, List&lt;ProducerBatch&gt;&gt; batches = <span class="keyword">this</span>.accumulator.drain(cluster, result.readyNodes, <span class="keyword">this</span>.maxRequestSize, now);</span><br><span class="line">        addToInflightBatches(batches);</span><br><span class="line">        <span class="keyword">if</span> (guaranteeMessageOrder) &#123;</span><br><span class="line">            <span class="comment">// Mute all the partitions drained</span></span><br><span class="line">            <span class="keyword">for</span> (List&lt;ProducerBatch&gt; batchList : batches.values()) &#123;</span><br><span class="line">                <span class="keyword">for</span> (ProducerBatch batch : batchList)</span><br><span class="line">                    <span class="keyword">this</span>.accumulator.mutePartition(batch.topicPartition);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 发送生产请求</span></span><br><span class="line">        sendProduceRequests(batches, now);</span><br><span class="line">        <span class="keyword">return</span> pollTimeout;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h4 id="1-3-3-addToInflightBatches"><a href="#1-3-3-addToInflightBatches" class="headerlink" title="1.3.3 addToInflightBatches"></a>1.3.3 addToInflightBatches</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// A per-partition queue of batches ordered by creation time for tracking the in-flight batches</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">final</span> Map&lt;TopicPartition, List&lt;ProducerBatch&gt;&gt; inFlightBatches;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">addToInflightBatches</span><span class="params">(List&lt;ProducerBatch&gt; batches)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (ProducerBatch batch : batches) &#123;</span><br><span class="line">        List&lt;ProducerBatch&gt; inflightBatchList =inFlightBatches.get(batch.topicPartition);</span><br><span class="line">        <span class="keyword">if</span> (inflightBatchList == <span class="keyword">null</span>) &#123;</span><br><span class="line">            inflightBatchList = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">            inFlightBatches.put(batch.topicPartition, inflightBatchList);</span><br><span class="line">        &#125;</span><br><span class="line">        inflightBatchList.add(batch);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Sender 从 <code>RecordAccumulator</code>中获取缓存的消息之后，会进一步将原本&lt;分区, Deque&lt; ProducerBatch&gt;&gt; 的保存形式转变成 <node, list< producerbatch> 的形式，其中 Node 表示 Kafka 集群的 broker 节点。对于网络连接来说，生产者客户端是与具体的 broker 节点建立的连接，也就是向具体的 broker 节点发送消息，而并不关心消息属于哪一个分区；而对于 <code>KafkaProducer</code> 的应用逻辑而言，我们只关注向哪个分区中发送哪些消息，所以在这里需要做一个应用逻辑层面到网络I/O层面的转换。</node,></p>
<p>在转换成 <node, list> 的形式之后，Sender 还会进一步封装成 <node, request> 的形式，这样就可以将 Request 请求发往各个 Node 了，这里的 Request 是指 Kafka 的各种协议请求，对于消息发送而言就是指具体的 <code>ProduceRequest</code>。</node,></node,></p>
<h3 id="1-4-NetworkClient"><a href="#1-4-NetworkClient" class="headerlink" title="1.4 NetworkClient"></a>1.4 NetworkClient</h3><h4 id="1-4-1-doSend"><a href="#1-4-1-doSend" class="headerlink" title="1.4.1 doSend"></a>1.4.1 doSend</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">doSend</span><span class="params">(ClientRequest clientRequest, <span class="keyword">boolean</span> isInternalRequest, <span class="keyword">long</span> now, AbstractRequest request)</span> </span>&#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">    Send send = request.toSend(destination, header);</span><br><span class="line">    InFlightRequest inFlightRequest = <span class="keyword">new</span> InFlightRequest(</span><br><span class="line">        clientRequest,</span><br><span class="line">        header,</span><br><span class="line">        isInternalRequest,</span><br><span class="line">        request,</span><br><span class="line">        send,</span><br><span class="line">        now);</span><br><span class="line">    <span class="keyword">this</span>.inFlightRequests.add(inFlightRequest);</span><br><span class="line">    selector.send(send);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>请求在从 Sender 线程发往 Kafka 之前还会保存到 <code>InFlightRequests</code> 中，InFlightRequests 保存对象的具体形式为 Map<nodeid, deque>，它的主要作用是缓存了已经发出去但还没有收到响应的请求（NodeId 是一个 String 类型，表示节点的 id 编号）。与此同时，<code>InFlightRequests</code>还提供了许多管理类的方法，并且通过配置参数还可以限制每个连接（也就是客户端与 Node 之间的连接）最多缓存的请求数。这个配置参数为 <code>max.in.flight.requests.per.connection</code>，默认值为5，即每个连接最多只能缓存5个未响应的请求，超过该数值之后就不能再向这个连接发送更多的请求了，除非有缓存的请求收到了响应（Response）。通过比较 Deque 的 size 与这个参数的大小来判断对应的 Node 中是否已经堆积了很多未响应的消息，如果真是如此，那么说明这个 Node 节点负载较大或网络连接有问题，再继续向其发送请求会增大请求超时的可能。</nodeid,></p>
<h2 id="2-元数据的更新"><a href="#2-元数据的更新" class="headerlink" title="2. 元数据的更新"></a>2. 元数据的更新</h2><p>前面提及的<code>InFlightRequests</code> 还可以获得 <code>leastLoadedNode</code>，即所有 Node 中负载最小的那一个。这里的负载最小是通过每个 Node 在 <code>InFlightRequests</code>中还未确认的请求决定的，未确认的请求越多则认为负载越大。对于下图中的 <code>InFlightRequests</code> 来说，图中展示了三个节点 Node0、Node1和Node2，很明显 Node1 的负载最小。也就是说，Node1 为当前的 <code>leastLoadedNode</code>。选择 <code>leastLoadedNode</code> 发送请求可以使它能够尽快发出，避免因网络拥塞等异常而影响整体的进度。<code>leastLoadedNode</code> 的概念可以用于多个应用场合，比如元数据请求、消费者组播协议的交互。</p>
<p><img src="/2019/05/03/Kafka系列/5.Kafka生产者客户端原理/2.png" alt></p>
<p>我们使用如下的方式创建了一条消息 <code>ProducerRecord</code>：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(topic, <span class="string">"Hello, Kafka!"</span>);</span><br></pre></td></tr></table></figure>
<p>我们只知道主题的名称，对于其他一些必要的信息却一无所知。<code>KafkaProducer</code> 要将此消息追加到指定主题的某个分区所对应的 leader 副本之前，首先需要知道主题的分区数量，然后经过计算得出（或者直接指定）目标分区，之后 <code>KafkaProducer</code> 需要知道目标分区的 leader 副本所在的 broker 节点的地址、端口等信息才能建立连接，最终才能将消息发送到 Kafka，在这一过程中所需要的信息都属于元数据信息。</p>
<p>我们了解了<code>bootstrap.servers</code> 参数只需要配置部分 broker 节点的地址即可，不需要配置所有 broker 节点的地址，因为客户端可以自己发现其他 broker 节点的地址，这一过程也属于元数据相关的更新操作。与此同时，分区数量及 leader 副本的分布都会动态地变化，客户端也需要动态地捕捉这些变化。</p>
<p>元数据是指 Kafka 集群的元数据，这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的 leader 副本分配在哪个节点上，follower 副本分配在哪些节点上，哪些副本在 AR、ISR 等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。</p>
<p>当客户端中没有需要使用的元数据信息时，比如没有指定的主题信息，或者超过 <code>metadata.max.age.ms</code>时间没有更新元数据都会引起元数据的更新操作。客户端参数 <code>metadata.max.age.ms</code> 的默认值为300000，即5分钟。元数据的更新操作是在客户端内部进行的，对客户端的外部使用者不可见。当需要更新元数据时，会先挑选出 <code>leastLoadedNode</code>，然后向这个 Node 发送 <code>MetadataRequest</code> 请求来获取具体的元数据信息。这个更新操作是由 Sender 线程发起的，在创建完 <code>MetadataRequest</code> 之后同样会存入 <code>InFlightRequests</code>，之后的步骤就和发送消息时的类似。元数据虽然由 Sender 线程负责更新，但是主线程也需要读取这些信息，这里的数据同步通过 synchronized 和 final 关键字来保障。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/03/Kafka系列/4.Kafka序列化-分区器-拦截器/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/03/Kafka系列/4.Kafka序列化-分区器-拦截器/" itemprop="url">Kafka序列化-分区器-拦截器</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-03T10:12:00+08:00">
                2019-05-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/03/Kafka系列/4.Kafka序列化-分区器-拦截器/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/03/Kafka系列/4.Kafka序列化-分区器-拦截器/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/03/Kafka系列/4.Kafka序列化-分区器-拦截器/" class="leancloud_visitors" data-flag-title="Kafka序列化-分区器-拦截器">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><img src="/2019/05/03/Kafka系列/4.Kafka序列化-分区器-拦截器/1.jpg" alt></p>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><p>生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给 Kafka。而在对侧，消费者需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。</p>
<p>在前面的代码中，为了方便，消息的 key 和 value 都使用了字符串，对应程序中的序列化器也使用了客户端自带的 <code>org.apache.kafka.common.serialization.StringSerializer</code>，除了用于 String 类型的序列化器，还有 <code>ByteArray</code>、<code>ByteBuffer</code>、<code>Bytes</code>、<code>Double</code>、<code>Integer</code>、<code>Long</code> 这几种类型，它们都实现了 <code>org.apache.kafka.common.serialization.Serializer</code> 接口，此接口有4个方法：</p>
<h3 id="1-Serializer"><a href="#1-Serializer" class="headerlink" title="1. Serializer"></a>1. Serializer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * An interface for converting objects to bytes.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * A class that implements this interface is expected to have a constructor with no parameter.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * Implement &#123;<span class="doctag">@link</span> org.apache.kafka.common.ClusterResourceListener&#125; to receive cluster metadata once it's available. Please see the class documentation for ClusterResourceListener for more information.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@param</span> &lt;T&gt; Type to be serialized from.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Serializer</span>&lt;<span class="title">T</span>&gt; <span class="keyword">extends</span> <span class="title">Closeable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Configure this class.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> configs configs in key/value pairs</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> isKey whether is for key or value</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Convert &#123;<span class="doctag">@code</span> data&#125; into a byte array.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic topic associated with data</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data typed data</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> serialized bytes</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">byte</span>[] serialize(String topic, T data);</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Convert &#123;<span class="doctag">@code</span> data&#125; into a byte array.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic topic associated with data</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> headers headers associated with the record</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> data typed data</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> serialized bytes</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="keyword">default</span> <span class="keyword">byte</span>[] serialize(String topic, Headers headers, T data) &#123;</span><br><span class="line">        <span class="keyword">return</span> serialize(topic, data);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Close this serializer.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * This method must be idempotent as it may be called multiple times.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>configure() 方法用来配置当前类，serialize() 方法用来执行序列化操作。而 close() 方法用来关闭当前的序列化器，一般情况下 close() 是一个空方法，如果实现了此方法，则必须确保此方法的幂等性，因为这个方法很可能会被 <code>KafkaProducer</code> 调用多次。</p>
<p>生产者使用的序列化器和消费者使用的反序列化器是需要一一对应的，如果生产者使用了某种序列化器，比如 <code>StringSerializer</code>，而消费者使用了另一种序列化器，比如 <code>IntegerSerializer</code>，那么是无法解析出想要的数据的。</p>
<h3 id="2-StringSerializer"><a href="#2-StringSerializer" class="headerlink" title="2. StringSerializer"></a>2. StringSerializer</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *  String encoding defaults to UTF8 and can be customized by setting the property key.serializer.encoding,</span></span><br><span class="line"><span class="comment"> *  value.serializer.encoding or serializer.encoding. The first two take precedence over the last.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StringSerializer</span> <span class="keyword">implements</span> <span class="title">Serializer</span>&lt;<span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String encoding = <span class="string">"UTF8"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line">        String propertyName = isKey ? <span class="string">"key.serializer.encoding"</span> : <span class="string">"value.serializer.encoding"</span>;</span><br><span class="line">        Object encodingValue = configs.get(propertyName);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue == <span class="keyword">null</span>)</span><br><span class="line">            encodingValue = configs.get(<span class="string">"serializer.encoding"</span>);</span><br><span class="line">        <span class="keyword">if</span> (encodingValue <span class="keyword">instanceof</span> String)</span><br><span class="line">            encoding = (String) encodingValue;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 就是将 String 类型转为 byte[] 类型。</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">byte</span>[] serialize(String topic, String data) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span> data.getBytes(encoding);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> SerializationException(<span class="string">"Error when serializing string to byte[] due to unsupported encoding "</span> + encoding);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// nothing to do</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>首先是 configure() 方法，这个方法是在创建<code>KafkaProducer</code> 实例的时候调用的，主要用来确定编码类型，不过一般客户端对于 <code>key.serializer.encoding</code>、<code>value.serializer. encoding</code> 和 <code>serializer.encoding</code>这几个参数都不会配置，在 <code>KafkaProducer</code> 的参数集合（<code>ProducerConfig</code>）里也没有这几个参数（它们可以看作用户自定义的参数），所以一般情况下 encoding 的值就为默认的“UTF-8”。serialize() 方法非常直观，就是将 String 类型转为 byte[] 类型。</p>
<h3 id="3-自定义Serializer"><a href="#3-自定义Serializer" class="headerlink" title="3. 自定义Serializer"></a>3. 自定义Serializer</h3><h4 id="3-1-数据准备"><a href="#3-1-数据准备" class="headerlink" title="3.1 数据准备"></a>3.1 数据准备</h4><h5 id="3-1-1-Message"><a href="#3-1-1-Message" class="headerlink" title="3.1.1  Message"></a>3.1.1  Message</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer.serializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Message</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">int</span> id;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String name;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Message</span><span class="params">(<span class="keyword">int</span> id, String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.id = id;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getId</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> id;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getName</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-1-2-MessageSerializer"><a href="#3-1-2-MessageSerializer" class="headerlink" title="3.1.2  MessageSerializer"></a>3.1.2  MessageSerializer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer.serializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serializer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.UnsupportedEncodingException;</span><br><span class="line"><span class="keyword">import</span> java.nio.ByteBuffer;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义消息序列化器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MessageSerializer</span> <span class="keyword">implements</span> <span class="title">Serializer</span>&lt;<span class="title">Message</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs, <span class="keyword">boolean</span> isKey)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">byte</span>[] serialize(String topic, Message data) &#123;</span><br><span class="line">        <span class="keyword">if</span> (data == <span class="keyword">null</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">int</span> id = data.getId();</span><br><span class="line">        String name = data.getName();</span><br><span class="line">        <span class="keyword">int</span> nameLength = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span> (name != <span class="keyword">null</span> &amp;&amp; !name.isEmpty()) &#123;</span><br><span class="line">            nameLength = name.length();</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 开启一个内存区间</span></span><br><span class="line">            ByteBuffer buffer = ByteBuffer.allocate(<span class="number">4</span> + <span class="number">4</span> + <span class="number">4</span> + nameLength);</span><br><span class="line">            buffer.putInt(<span class="number">4</span>);</span><br><span class="line">            buffer.putInt(id);</span><br><span class="line">            buffer.putInt(<span class="number">4</span>);</span><br><span class="line">            buffer.put(name.getBytes(<span class="string">"UTF-8"</span>));</span><br><span class="line">            <span class="keyword">return</span> buffer.array();</span><br><span class="line">        &#125; <span class="keyword">catch</span> (UnsupportedEncodingException e) &#123;</span><br><span class="line">            <span class="comment">// 编码不支持异常</span></span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">0</span>];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="3-1-3-Producer"><a href="#3-1-3-Producer" class="headerlink" title="3.1.3 Producer"></a>3.1.3 Producer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer.serializer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Future;</span><br><span class="line"><span class="keyword">import</span> java.util.stream.IntStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MySerializerProducerDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(MySerializerProducerDemo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = initProps();</span><br><span class="line">        <span class="comment">// 值改成自定义的Message</span></span><br><span class="line">        KafkaProducer&lt;String, Message&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        IntStream.range(<span class="number">0</span>, <span class="number">10</span>).forEach(i -&gt;</span><br><span class="line">        &#123;</span><br><span class="line">            ProducerRecord&lt;String, Message&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"send-demo-topic"</span>, String.valueOf(i), <span class="keyword">new</span> Message(<span class="number">10</span>, <span class="string">"test"</span>));</span><br><span class="line">            Future&lt;RecordMetadata&gt; future = producer.send(record);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                RecordMetadata metaData = future.get();</span><br><span class="line">                LOGGER.info(<span class="string">"The message is send done and the key is &#123;&#125;,offset &#123;&#125;"</span>, i, metaData.offset());</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initProps</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.78:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        <span class="comment">// 修改成自定义的MessageSerializer</span></span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, MessageSerializer.class.getName());</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="分区器"><a href="#分区器" class="headerlink" title="分区器"></a>分区器</h2><p>消息在通过 send() 方法发往 broker 的过程中，有可能需要经过拦截器（<code>Interceptor</code>）、序列化器（<code>Serializer</code>）和分区器（<code>Partitioner</code>）的一系列作用之后才能被真正地发往 broker。拦截器一般不是必需的，而序列化器是必需的。消息经过序列化之后就需要确定它发往的分区，如果消息<code>ProducerRecord</code> 中指定了 partition 字段，那么就不需要分区器的作用，因为 partition 代表的就是所要发往的分区号。</p>
<p>如果消息 <code>ProducerRecord</code> 中没有指定 partition 字段，那么就需要依赖分区器，根据 key 这个字段来计算 partition 的值。分区器的作用就是为消息分配分区。</p>
<p>Kafka 中提供的默认分区器是 <code>org.apache.kafka.clients.producer.internals.DefaultPartitioner</code>。</p>
<h3 id="1-Configurable"><a href="#1-Configurable" class="headerlink" title="1. Configurable"></a>1. Configurable</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A Mix-in style interface for classes that are instantiated by reflection and need to take configuration parameters</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Configure this class with the given key-value pairs</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-Partitioner"><a href="#2-Partitioner" class="headerlink" title="2. Partitioner"></a>2. Partitioner</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Partitioner Interface</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Partitioner</span> <span class="keyword">extends</span> <span class="title">Configurable</span>, <span class="title">Closeable</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic The topic name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes The serialized key to partition on( or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes The serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * This is called when partitioner is closed.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="3-DefaultPartitioner"><a href="#3-DefaultPartitioner" class="headerlink" title="3. DefaultPartitioner"></a>3. DefaultPartitioner</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The default partitioning strategy:</span></span><br><span class="line"><span class="comment"> * &lt;ul&gt;</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If a partition is specified in the record, use it</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If no partition is specified but a key is present choose a partition based on a hash of the key</span></span><br><span class="line"><span class="comment"> * &lt;li&gt;If no partition or key is present choose a partition in a round-robin fashion</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DefaultPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> ConcurrentMap&lt;String, AtomicInteger&gt; topicCounterMap = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic The topic name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value The value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);</span><br><span class="line">        <span class="keyword">int</span> numPartitions = partitions.size();</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">int</span> nextValue = nextValue(topic);</span><br><span class="line">            List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">            <span class="keyword">if</span> (availablePartitions.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                <span class="keyword">int</span> part = Utils.toPositive(nextValue) % availablePartitions.size();</span><br><span class="line">                <span class="keyword">return</span> availablePartitions.get(part).partition();</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="comment">// no partitions are available, give a non-available partition</span></span><br><span class="line">                <span class="keyword">return</span> Utils.toPositive(nextValue) % numPartitions;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="comment">// hash the keyBytes to choose a partition</span></span><br><span class="line">            <span class="keyword">return</span> Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">nextValue</span><span class="params">(String topic)</span> </span>&#123;</span><br><span class="line">        AtomicInteger counter = topicCounterMap.get(topic);</span><br><span class="line">        <span class="keyword">if</span> (<span class="keyword">null</span> == counter) &#123;</span><br><span class="line">            counter = <span class="keyword">new</span> AtomicInteger(ThreadLocalRandom.current().nextInt());</span><br><span class="line">            AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);</span><br><span class="line">            <span class="keyword">if</span> (currentCounter != <span class="keyword">null</span>) &#123;</span><br><span class="line">                counter = currentCounter;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> counter.getAndIncrement();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="4-自定义Partitioner"><a href="#4-自定义Partitioner" class="headerlink" title="4. 自定义Partitioner"></a>4. 自定义Partitioner</h3><h4 id="4-1-数据准备"><a href="#4-1-数据准备" class="headerlink" title="4.1 数据准备"></a>4.1 数据准备</h4><h5 id="4-1-1-MyPartitioner"><a href="#4-1-1-MyPartitioner" class="headerlink" title="4.1.1 MyPartitioner"></a>4.1.1 MyPartitioner</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer.partition;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.Cluster;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义Partitioner</span></span><br><span class="line"><span class="comment"> * &#123;<span class="doctag">@link</span> Partitioner&#125;</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">implements</span> <span class="title">Partitioner</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String ONE = <span class="string">"ONE"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String TWO = <span class="string">"TWO"</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String THREE = <span class="string">"THREE"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Compute the partition for the given record.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> topic      The topic name</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> key        The key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> keyBytes   serialized key to partition on (or null if no key)</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> value      The value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> valueBytes serialized value to partition on or null</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> cluster    The current cluster metadata</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">partition</span><span class="params">(String topic, Object key, <span class="keyword">byte</span>[] keyBytes, Object value, <span class="keyword">byte</span>[] valueBytes, Cluster cluster)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (keyBytes == <span class="keyword">null</span> || keyBytes.length == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"The key is required for BIZ."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">switch</span> (key.toString().toUpperCase()) &#123;</span><br><span class="line">            <span class="keyword">case</span> ONE:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">case</span> TWO:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">case</span> THREE:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> IllegalArgumentException(<span class="string">"The key is invalid."</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="4-1-2-Producer"><a href="#4-1-2-Producer" class="headerlink" title="4.1.2 Producer"></a>4.1.2 Producer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhang.producer.partition.MyPartitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Future;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义分区生产者Demo</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">PartitionProducerDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(PartitionProducerDemo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">        Properties properties = initProps();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line"><span class="comment">//        ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("partition-demo-topic", "ONE", "hello ONE");</span></span><br><span class="line"><span class="comment">//      ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("partition-demo-topic", "TWO", "hello TWO");</span></span><br><span class="line"><span class="comment">//      ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("partition-demo-topic", "THREE", "hello THREE");</span></span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"partition-demo-topic"</span>, <span class="string">"FOUR"</span>, <span class="string">"hello FOUR"</span>);</span><br><span class="line">        Future&lt;RecordMetadata&gt; future = producer.send(record);</span><br><span class="line">        RecordMetadata recordMetadata = future.get();</span><br><span class="line">        LOGGER.info(<span class="string">"当前数据的partition为 &#123;&#125;"</span>, recordMetadata.partition());</span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initProps</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.78:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.PARTITIONER_CLASS_CONFIG, MyPartitioner.class.getName());</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="生产者拦截器"><a href="#生产者拦截器" class="headerlink" title="生产者拦截器"></a>生产者拦截器</h2><p>拦截器（Interceptor）是早在 Kafka 0.10.0.0 中就已经引入的一个功能，Kafka 一共有两种拦截器：生产者拦截器和消费者拦截器。</p>
<p>生产者拦截器既可以用来在消息发送前做一些准备工作，比如按照某个规则过滤不符合要求的消息、修改消息的内容等，也可以用来在发送回调逻辑前做一些定制化的需求，比如统计类工作。</p>
<h3 id="1-ProducerInterceptor"><a href="#1-ProducerInterceptor" class="headerlink" title="1. ProducerInterceptor"></a>1. ProducerInterceptor</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * A plugin interface that allows you to intercept (and possibly mutate) the records received by the producer before</span></span><br><span class="line"><span class="comment"> * they are published to the Kafka cluster.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * This class will get producer config properties via &lt;code&gt;configure()&lt;/code&gt; method, including clientId assigned</span></span><br><span class="line"><span class="comment"> * by KafkaProducer if not specified in the producer config. The interceptor implementation needs to be aware that it will be</span></span><br><span class="line"><span class="comment"> * sharing producer config namespace with other interceptors and serializers, and ensure that there are no conflicts.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * Exceptions thrown by ProducerInterceptor methods will be caught, logged, but not propagated further. As a result, if</span></span><br><span class="line"><span class="comment"> * the user configures the interceptor with the wrong key and value type parameters, the producer will not throw an exception,</span></span><br><span class="line"><span class="comment"> * just log the errors.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * ProducerInterceptor callbacks may be called from multiple threads. Interceptor implementation must ensure thread-safety, if needed.</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * Implement &#123;<span class="doctag">@link</span> org.apache.kafka.common.ClusterResourceListener&#125; to receive cluster metadata once it's available. Please see the class documentation for ClusterResourceListener for more information.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; <span class="keyword">extends</span> <span class="title">Configurable</span> </span>&#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * This is called from &#123;<span class="doctag">@link</span> org.apache.kafka.clients.producer.KafkaProducer#send(ProducerRecord)&#125; and</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> org.apache.kafka.clients.producer.KafkaProducer#send(ProducerRecord, Callback)&#125; methods, before key and value</span></span><br><span class="line"><span class="comment">     * get serialized and partition is assigned (if partition is not specified in ProducerRecord).</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * This method is allowed to modify the record, in which case, the new record will be returned. The implication of modifying</span></span><br><span class="line"><span class="comment">     * key/value is that partition assignment (if not specified in ProducerRecord) will be done based on modified key/value,</span></span><br><span class="line"><span class="comment">     * not key/value from the client. Consequently, key and value transformation done in onSend() needs to be consistent:</span></span><br><span class="line"><span class="comment">     * same key and value should mutate to the same (modified) key and value. Otherwise, log compaction would not work</span></span><br><span class="line"><span class="comment">     * as expected.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Similarly, it is up to interceptor implementation to ensure that correct topic/partition is returned in ProducerRecord.</span></span><br><span class="line"><span class="comment">     * Most often, it should be the same topic/partition from 'record'.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Any exception thrown by this method will be caught by the caller and logged, but not propagated further.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Since the producer may run multiple interceptors, a particular interceptor's onSend() callback will be called in the order</span></span><br><span class="line"><span class="comment">     * specified by &#123;<span class="doctag">@link</span> org.apache.kafka.clients.producer.ProducerConfig#INTERCEPTOR_CLASSES_CONFIG&#125;. The first interceptor</span></span><br><span class="line"><span class="comment">     * in the list gets the record passed from the client, the following interceptor will be passed the record returned by the</span></span><br><span class="line"><span class="comment">     * previous interceptor, and so on. Since interceptors are allowed to modify records, interceptors may potentially get</span></span><br><span class="line"><span class="comment">     * the record already modified by other interceptors. However, building a pipeline of mutable interceptors that depend on the output</span></span><br><span class="line"><span class="comment">     * of the previous interceptor is discouraged, because of potential side-effects caused by interceptors potentially failing to</span></span><br><span class="line"><span class="comment">     * modify the record and throwing an exception. If one of the interceptors in the list throws an exception from onSend(), the exception</span></span><br><span class="line"><span class="comment">     * is caught, logged, and the next interceptor is called with the record returned by the last successful interceptor in the list,</span></span><br><span class="line"><span class="comment">     * or otherwise the client.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> record the record from client or the record returned by the previous interceptor in the chain of interceptors.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@return</span> producer record to send to topic/partition</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;K, V&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;K, V&gt; record)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * This method is called when the record sent to the server has been acknowledged, or when sending the record fails before</span></span><br><span class="line"><span class="comment">     * it gets sent to the server.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * This method is generally called just before the user callback is called, and in additional cases when &lt;code&gt;KafkaProducer.send()&lt;/code&gt;</span></span><br><span class="line"><span class="comment">     * throws an exception.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * Any exception thrown by this method will be ignored by the caller.</span></span><br><span class="line"><span class="comment">     * &lt;p&gt;</span></span><br><span class="line"><span class="comment">     * This method will generally execute in the background I/O thread, so the implementation should be reasonably fast.</span></span><br><span class="line"><span class="comment">     * Otherwise, sending of messages from other threads could be delayed.</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> metadata The metadata for the record that was sent (i.e. the partition and offset).</span></span><br><span class="line"><span class="comment">     *                 If an error occurred, metadata will contain only valid topic and maybe</span></span><br><span class="line"><span class="comment">     *                 partition. If partition is not given in ProducerRecord and an error occurs</span></span><br><span class="line"><span class="comment">     *                 before partition gets assigned, then partition will be set to RecordMetadata.NO_PARTITION.</span></span><br><span class="line"><span class="comment">     *                 The metadata may be null if the client passed null record to</span></span><br><span class="line"><span class="comment">     *                 &#123;<span class="doctag">@link</span> org.apache.kafka.clients.producer.KafkaProducer#send(ProducerRecord)&#125;.</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> exception The exception thrown during processing of this record. Null if no error occurred.</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * This is called when interceptor is closed</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>KafkaProducer 在将消息序列化和计算分区之前会调用生产者拦截器的 onSend() 方法来对消息进行相应的定制化操作。一般来说最好不要修改消息 ProducerRecord 的 topic、key 和 partition 等信息，如果要修改，则需确保对其有准确的判断，否则会与预想的效果出现偏差。比如修改 key 不仅会影响分区的计算，同样会影响 broker 端日志压缩（Log Compaction）的功能。</p>
<p>KafkaProducer 会在消息被应答（Acknowledgement）之前或消息发送失败时调用生产者拦截器的 onAcknowledgement() 方法，优先于用户设定的 Callback 之前执行。这个方法运行在 Producer 的I/O线程中，所以这个方法中实现的代码逻辑越简单越好，否则会影响消息的发送速度。</p>
<p>close() 方法主要用于在关闭拦截器时执行一些资源的清理工作。在这3个方法中抛出的异常都会被捕获并记录到日志中，但并不会再向上传递。</p>
<h3 id="2-自定义ProducerInterceptor"><a href="#2-自定义ProducerInterceptor" class="headerlink" title="2. 自定义ProducerInterceptor"></a>2. 自定义ProducerInterceptor</h3><h4 id="2-1-数据准备"><a href="#2-1-数据准备" class="headerlink" title="2.1 数据准备"></a>2.1 数据准备</h4><h5 id="2-1-1-ProducerInterceptorPrefix"><a href="#2-1-1-ProducerInterceptorPrefix" class="headerlink" title="2.1.1 ProducerInterceptorPrefix"></a>2.1.1 ProducerInterceptorPrefix</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer.interceptor;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerInterceptor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 前缀拦截器</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerInterceptorPrefix</span> <span class="keyword">implements</span> <span class="title">ProducerInterceptor</span>&lt;<span class="title">String</span>, <span class="title">String</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> sendSuccess = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">volatile</span> <span class="keyword">long</span> sendFailure = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> ProducerRecord&lt;String, String&gt; <span class="title">onSend</span><span class="params">(ProducerRecord&lt;String, String&gt; record)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 给value前面加个前缀</span></span><br><span class="line">        String newValue = <span class="string">"producer-"</span> + record.value();</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> ProducerRecord&lt;&gt;(record.topic(), record.partition(), record.timestamp(), record.key(), newValue, record.headers());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onAcknowledgement</span><span class="params">(RecordMetadata metadata, Exception exception)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span>(exception == <span class="keyword">null</span>)&#123;</span><br><span class="line">            sendSuccess++;</span><br><span class="line">        &#125; <span class="keyword">else</span>&#123;</span><br><span class="line">            sendFailure++;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">close</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 统计发送成功率</span></span><br><span class="line">        <span class="keyword">double</span> successRatio = (<span class="keyword">double</span>)sendSuccess / (sendFailure + sendSuccess);</span><br><span class="line">        System.out.println(<span class="string">"[INFO] 发送成功率="</span> + String.format(<span class="string">"%f"</span>, successRatio * <span class="number">100</span>) + <span class="string">"%"</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">configure</span><span class="params">(Map&lt;String, ?&gt; configs)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h5 id="2-2-Producer"><a href="#2-2-Producer" class="headerlink" title="2.2 Producer"></a>2.2 Producer</h5><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> com.zhang.producer.interceptor.ProducerInterceptorPrefix;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.StringSerializer;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.stream.IntStream;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 生产拦截器测试</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerInterceptorDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(ProducerInterceptorDemo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = initProps();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        IntStream.range(<span class="number">0</span>, <span class="number">10</span>).forEach(i -&gt; &#123;</span><br><span class="line">            producer.send(<span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"interceptor-demo-topic"</span>, String.valueOf(i), <span class="string">"interceptor "</span> + i));</span><br><span class="line">            LOGGER.info(<span class="string">"&#123;&#125; message send done ..."</span>, i);</span><br><span class="line">        &#125;);</span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initProps</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.117:9092,10.153.1.128:9092,10.153.1.131:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">        <span class="comment">// 配置拦截器</span></span><br><span class="line">        props.put(ProducerConfig.INTERCEPTOR_CLASSES_CONFIG, ProducerInterceptorPrefix.class.getName());</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果拦截链中的某个拦截器的执行需要依赖于前一个拦截器的输出，那么就有可能产生“副作用”。设想一下，如果前一个拦截器由于异常而执行失败，那么这个拦截器也就跟着无法继续执行。在拦截链中，如果某个拦截器执行失败，那么下一个拦截器会接着从上一个执行成功的拦截器继续执行。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/02/Kafka系列/3.Kafka生产者开发/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/02/Kafka系列/3.Kafka生产者开发/" itemprop="url">Kafka生产者开发</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-02T19:31:00+08:00">
                2019-05-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/02/Kafka系列/3.Kafka生产者开发/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/02/Kafka系列/3.Kafka生产者开发/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/02/Kafka系列/3.Kafka生产者开发/" class="leancloud_visitors" data-flag-title="Kafka生产者开发">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>从编程的角度而言，生产者就是负责向 Kafka 发送消息的应用程序。在 Kafka 的历史变迁中，一共有两个大版本的生产者客户端：第一个是于 Kafka 开源之初使用 Scala 语言编写的客户端，我们可以称之为旧生产者客户端（Old Producer）或 Scala 版生产者客户端；第二个是从 Kafka 0.9.x 版本开始推出的使用 Java 语言编写的客户端，我们可以称之为新生产者客户端（New Producer）或 Java 版生产者客户端，它弥补了旧版客户端中存在的诸多设计缺陷。</p>
<h2 id="客户端开发"><a href="#客户端开发" class="headerlink" title="客户端开发"></a>客户端开发</h2><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * kafka 生产者demo</span></span><br><span class="line"><span class="comment"> * &lt;p&gt;</span></span><br><span class="line"><span class="comment"> * 1. 配置生产者客户端参数及创建相应的生产者实例。</span></span><br><span class="line"><span class="comment"> * 2. 构建待发送的消息。</span></span><br><span class="line"><span class="comment"> * 3. 发送消息。</span></span><br><span class="line"><span class="comment"> * 4. 关闭生产者实例。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String brokerList = <span class="string">"10.153.1.78:9092"</span>;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String topic = <span class="string">"topic-demo"</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Properties <span class="title">initConfig</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, brokerList);</span><br><span class="line">        props.put(<span class="string">"key.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.serializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(<span class="string">"client.id"</span>, <span class="string">"producer.client.id.demo"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties props = initConfig();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(props);</span><br><span class="line">        ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(topic, <span class="string">"Hello, Kafka!"</span>);</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            producer.send(record);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里有必要单独说明的是构建的消息对象 <code>ProducerRecord</code>，它并不是单纯意义上的消息，它包含了多个属性，原本需要发送的与业务相关的消息体只是其中的一个 value 属性，比如“Hello, Kafka!”只是 <code>ProducerRecord</code> 对象中的一个属性。<code>ProducerRecord</code> 类的定义如下（只截取成员变量）</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerRecord</span>&lt;<span class="title">K</span>, <span class="title">V</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> String topic; <span class="comment">//主题</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Integer partition; <span class="comment">//分区号</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Headers headers; <span class="comment">//消息头部</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> K key; <span class="comment">//键</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> V value; <span class="comment">//值</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> Long timestamp; <span class="comment">//消息的时间戳</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中 topic 和 partition 字段分别代表消息要发往的主题和分区号。headers 字段是消息的头部，Kafka 0.11.x 版本才引入这个属性，它大多用来设定一些与应用相关的信息，如无需要也可以不用设置。key 是用来指定消息的键，它不仅是消息的附加信息，还可以用来计算分区号进而可以让消息发往特定的分区。前面提及消息以主题为单位进行归类，而这个 key 可以让消息再进行二次归类，同一个 key 的消息会被划分到同一个分区中。</p>
<p>有 key 的消息还可以支持日志压缩的功能。value 是指消息体，一般不为空，如果为空则表示特定的消息—墓碑消息。timestamp 是指消息的时间戳，它有 CreateTime 和 LogAppendTime 两种类型，前者表示消息创建的时间，后者表示消息追加到日志文件的时间。</p>
<h2 id="必要的参数配置"><a href="#必要的参数配置" class="headerlink" title="必要的参数配置"></a>必要的参数配置</h2><ul>
<li><p><strong>bootstrap.servers</strong></p>
<p>该参数用来指定生产者客户端连接 Kafka 集群所需的 broker 地址清单，具体的内容格式为 host1:port1,host2:port2，可以设置一个或多个地址，中间以逗号隔开，此参数的默认值为“”。注意这里并非需要所有的 broker 地址，因为生产者会从给定的 broker 里查找到其他 broker 的信息。不过建议至少要设置两个以上的 broker 地址信息，当其中任意一个宕机时，生产者仍然可以连接到 Kafka 集群上。</p>
</li>
<li><p><strong>key.serializer</strong></p>
<p>将用于序列化我们将生成到Kafka的记录的键的类的名称。 Kafka brokers希望字节数组作为消息的键和值。但是, 生成器接口允许使用参数化类型发送任何 Java 对象作为键和值。</p>
</li>
<li><p><strong>value.serializer</strong></p>
<p>将用于序列化我们将生成到Kafka的记录的值的类的名称。</p>
</li>
</ul>
<h2 id="消息的发送"><a href="#消息的发送" class="headerlink" title="消息的发送"></a>消息的发送</h2><h3 id="发后即忘"><a href="#发后即忘" class="headerlink" title="发后即忘"></a>发后即忘</h3><p>我们向服务器发送消息, 并不真正关心它是否成功到达。大多数情况下, 它将成功到达, 因为Kafka 是高可用的, 并且生成器将自动重试发送消息。但是, 某些情况下将丢失消息。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.stream.IntStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FireAndForgetProducerDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(FireAndForgetProducerDemo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = initProps();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        IntStream.range(<span class="number">0</span>, <span class="number">10</span>).forEach(i -&gt;</span><br><span class="line">        &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"send-demo-topic"</span>, String.valueOf(i), <span class="string">"hello "</span> + i);</span><br><span class="line">            producer.send(record);</span><br><span class="line">            LOGGER.info(<span class="string">"The message is send done and the key is &#123;&#125;"</span>, i);</span><br><span class="line">        &#125;);</span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initProps</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.78:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="同步发送"><a href="#同步发送" class="headerlink" title="同步发送"></a>同步发送</h3><p>我们发送一条消息, send () 方法返回一个 Future 对象, 我们使用 get () 等待将来, 看看 send () 是否成功。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.RecordMetadata;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.ExecutionException;</span><br><span class="line"><span class="keyword">import</span> java.util.concurrent.Future;</span><br><span class="line"><span class="keyword">import</span> java.util.stream.IntStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SyncProducerDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(SyncProducerDemo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = initProps();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        IntStream.range(<span class="number">0</span>, <span class="number">10</span>).forEach(i -&gt;</span><br><span class="line">        &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"send-demo-topic"</span>, String.valueOf(i), <span class="string">"hello "</span> + i);</span><br><span class="line">            Future&lt;RecordMetadata&gt; future = producer.send(record);</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                RecordMetadata metaData = future.get();</span><br><span class="line">                LOGGER.info(<span class="string">"The message is send done and the key is &#123;&#125;,offset &#123;&#125;"</span>, i, metaData.offset());</span><br><span class="line">            &#125; <span class="keyword">catch</span> (InterruptedException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (ExecutionException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;);</span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initProps</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.78:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="异步发送"><a href="#异步发送" class="headerlink" title="异步发送"></a>异步发送</h3><p>我们使用回调函数调用 send () 方法, 当它收到来自Kafka 服务器的响应时, 该方法将被触发。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.stream.IntStream;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">AsyncProducerDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(AsyncProducerDemo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = initProps();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        IntStream.range(<span class="number">0</span>, <span class="number">10</span>).forEach(i -&gt;</span><br><span class="line">        &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"send-demo-topic"</span>, String.valueOf(i), <span class="string">"hello "</span> + i);</span><br><span class="line">            producer.send(record, (metadata, exception) -&gt; &#123;</span><br><span class="line">                <span class="keyword">if</span> (exception == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    LOGGER.info(<span class="string">"The message is send done and the key is &#123;&#125;,offset &#123;&#125;"</span>, i, metadata.offset());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">        &#125;);</span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initProps</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.78:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="重要的生产者参数"><a href="#重要的生产者参数" class="headerlink" title="重要的生产者参数"></a>重要的生产者参数</h2><h3 id="acks"><a href="#acks" class="headerlink" title="acks"></a>acks</h3><p>这个参数用来指定分区中必须要有多少个副本收到这条消息，之后生产者才会认为这条消息是成功写入的。acks 是生产者客户端中一个非常重要的参数，它涉及消息的可靠性和吞吐量之间的权衡。acks 参数有3种类型的值（都是字符串类型）。</p>
<ul>
<li>acks = 1。默认值即为1。生产者发送消息之后，只要分区的 leader 副本成功写入消息，那么它就会收到来自服务端的成功响应。如果消息无法写入 leader 副本，比如在 leader 副本崩溃、重新选举新的 leader 副本的过程中，那么生产者就会收到一个错误的响应，为了避免消息丢失，生产者可以选择重发消息。如果消息写入 leader 副本并返回成功响应给生产者，且在被其他 follower 副本拉取之前 leader 副本崩溃，那么此时消息还是会丢失，因为新选举的 leader 副本中并没有这条对应的消息。acks 设置为1，是消息可靠性和吞吐量之间的折中方案。</li>
<li>acks = 0。生产者发送消息之后不需要等待任何服务端的响应。如果在消息从发送到写入 Kafka 的过程中出现某些异常，导致 Kafka 并没有收到这条消息，那么生产者也无从得知，消息也就丢失了。在其他配置环境相同的情况下，acks 设置为0可以达到最大的吞吐量。</li>
<li>acks = -1 或 acks = all。生产者在消息发送之后，需要等待 ISR 中的所有副本都成功写入消息之后才能够收到来自服务端的成功响应。在其他配置环境相同的情况下，acks 设置为 -1（all） 可以达到最强的可靠性。但这并不意味着消息就一定可靠，因为ISR中可能只有 leader 副本，这样就退化成了 acks=1 的情况。要获得更高的消息可靠性需要配合 min.insync.replicas 等参数的联动。</li>
</ul>
<h3 id="max-request-size"><a href="#max-request-size" class="headerlink" title="max.request.size"></a>max.request.size</h3><p>这个参数用来限制生产者客户端能发送的消息的最大值，默认值为1048576B，即1MB。一般情况下，这个默认值就可以满足大多数的应用场景了。</p>
<p>笔者并不建议读者盲目地增大这个参数的配置值，尤其是在对 Kafka 整体脉络没有足够把控的时候。因为这个参数还涉及一些其他参数的联动，比如 broker 端的 message.max.bytes 参数，如果配置错误可能会引起一些不必要的异常。比如将 broker 端的 message.max.bytes 参数配置为10，而 max.request.size 参数配置为20，那么当我们发送一条大小为15B的消息时，生产者客户端就会报出如下的异常：</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">org.apache.kafka.common.errors.RecordTooLargeException: The request included a message larger than the max message size the server will accept.</span><br></pre></td></tr></table></figure>
<h3 id="retries"><a href="#retries" class="headerlink" title="retries"></a>retries</h3><p>retries 参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作。消息在从生产者发出到成功写入服务器之前可能发生一些临时性的异常，比如网络抖动、leader 副本的选举等，这种异常往往是可以自行恢复的，生产者可以通过配置 retries 大于0的值，以此通过内部重试来恢复而不是一味地将异常抛给生产者的应用程序。如果重试达到设定的次数，那么生产者就会放弃重试并返回异常。不过并不是所有的异常都是可以通过重试来解决的，比如消息太大，超过 max.request.size 参数配置的值时，这种方式就不可行了。</p>
<p>重试还和另一个参数 retry.backoff.ms 有关，这个参数的默认值为100，它用来设定两次重试之间的时间间隔，避免无效的频繁重试。在配置 retries 和 retry.backoff.ms 之前，最好先估算一下可能的异常恢复时间，这样可以设定总的重试时间大于这个异常恢复时间，以此来避免生产者过早地放弃重试。</p>
<p>Kafka 可以保证同一个分区中的消息是有序的。如果生产者按照一定的顺序发送消息，那么这些消息也会顺序地写入分区，进而消费者也可以按照同样的顺序消费它们。</p>
<p>对于某些应用来说，顺序性非常重要，比如 MySQL 的 binlog 传输，如果出现错误就会造成非常严重的后果。如果将acks参数配置为非零值，并且 max.in.flight.requests.per.connection 参数配置为大于1的值，那么就会出现错序的现象：如果第一批次消息写入失败，而第二批次消息写入成功，那么生产者会重试发送第一批次的消息，此时如果第一批次的消息写入成功，那么这两个批次的消息就出现了错序。一般而言，在需要保证消息顺序的场合建议把参数 max.in.flight.requests.per.connection 配置为1，而不是把 acks 配置为0，不过这样也会影响整体的吞吐。</p>
<h3 id="buffer-memory"><a href="#buffer-memory" class="headerlink" title="buffer.memory"></a>buffer.memory</h3><p>生成器可用于缓冲等待发送到服务器的记录的内存总字节数。如果记录的发送速度快于将其传递到服务器的速度, 则生成器将阻止 max.block.ms 之后将引发异常。</p>
<p>此设置应大致对应于生成器将使用的总内存, 但不是硬绑定, 因为不是生成器使用的所有内存都用于缓冲。一些额外的内存将用于压缩 (如果启用压缩) 以及维护正在运行的请求。</p>
<h3 id="max-block-ms"><a href="#max-block-ms" class="headerlink" title="max.block.ms"></a>max.block.ms</h3><p>此参数控制生成器在调用 send () 和通过分区 partitionsFor() 。显式请求元数据时阻止的时间。当生成器的发送缓冲区已满或元数据不可用时, 这些方法将被阻止。当达到 max.block.ms 时, 将引发超时异常。</p>
<h3 id="batch-size"><a href="#batch-size" class="headerlink" title="batch.size"></a>batch.size</h3><p>当多个记录发送到同一个分区时, 生成器将对它们进行批处理。此参数控制将用于每个批处理的内存量 (以字节为单位) (而不是消息!)。批处理满后, 将发送批处理中的所有消息。但是, 这并不意味着生产者将等待批处理满。生产者将发送半批次, 甚至批次,中只有一条消息。因此, 将批处理大小设置得过大不会导致发送消息的延迟;它将只是使用更多的内存的批次。将批处理大小设置得过小将增加一些开销, 因为生产者需要更频繁地发送消息。</p>
<h3 id="client-id"><a href="#client-id" class="headerlink" title="client.id"></a>client.id</h3><p>这可以是任何字符串, 并将由代理用于标识从客户端发送的消息。它用于日志记录和指标。</p>
<h3 id="compression-type"><a href="#compression-type" class="headerlink" title="compression.type"></a>compression.type</h3><p>这个参数用来指定消息的压缩方式，默认值为“none”，即默认情况下，消息不会被压缩。该参数还可以配置为“gzip”“snappy”和“lz4”。对消息进行压缩可以极大地减少网络传输量、降低网络I/O，从而提高整体的性能。消息压缩是一种使用时间换空间的优化方式，如果对时延有一定的要求，则不推荐对消息进行压缩。</p>
<p>Snappy 压缩是由 Google 发明的, 目的是提供良好的压缩比, 具有较低的 CPU 开销和良好的性能, 因此建议在性能和带宽都值得关注的情况下使用。Gzip 压缩通常会使用更多的 CPU 和时间, 但会产生更好的压缩比, 因此建议在网络带宽受到更多限制的情况下使用。通过启用压缩, 可以降低网络利用率和存储, 这通常是向Kafka发送消息时的瓶颈。</p>
<h3 id="connections-max-idle-ms"><a href="#connections-max-idle-ms" class="headerlink" title="connections.max.idle.ms"></a>connections.max.idle.ms</h3><p>这个参数用来指定在多久之后关闭闲置的连接，默认值是540000（ms），即9分钟。</p>
<h3 id="linger-ms"><a href="#linger-ms" class="headerlink" title="linger.ms"></a>linger.ms</h3><p>当多个记录发送到同一个分区时, 生成器将对它们进行批处理。此参数控制将用于每个批处理的内存量 (以字节为单位) (而不是消息!)。批处理满后, 将发送批处理中的所有消息。但是, 这并不意味着生成器将等待批处理满。生产商将发送半满批次, 甚至批次, 其中只有一条消息。因此, 将批处理大小设置得过大不会导致发送消息的延迟;它将只是使用更多的内存的批次。将批处理大小设置得过小将增加一些开销, 因为生成器需要更频繁地发送消息。</p>
<h3 id="receive-buffer-bytes"><a href="#receive-buffer-bytes" class="headerlink" title="receive.buffer.bytes"></a>receive.buffer.bytes</h3><p>这个参数用来设置 Socket 接收消息缓冲区（SO_RECBUF）的大小，默认值为32768（B），即32KB。如果设置为-1，则使用操作系统的默认值。如果 Producer 与 Kafka 处于不同的机房，则可以适地调大这个参数值。</p>
<h3 id="send-buffer-bytes"><a href="#send-buffer-bytes" class="headerlink" title="send.buffer.bytes"></a>send.buffer.bytes</h3><p>这个参数用来设置 Socket 发送消息缓冲区（SO_SNDBUF）的大小，默认值为131072（B），即128KB。与 receive.buffer.bytes 参数一样，如果设置为-1，则使用操作系统的默认值。</p>
<h3 id="request-timeout-ms"><a href="#request-timeout-ms" class="headerlink" title="request.timeout.ms"></a>request.timeout.ms</h3><p>这个参数用来配置 Producer 等待请求响应的最长时间，默认值为30000（ms）。请求超时之后可以选择进行重试。注意这个参数需要比 broker 端参数 replica.lag.time.max.ms 的值要大，这样可以减少因客户端重试而引起的消息重复的概率。</p>
<h3 id="max-in-flight-requests-per-connection"><a href="#max-in-flight-requests-per-connection" class="headerlink" title="max.in.flight.requests.per.connection"></a>max.in.flight.requests.per.connection</h3><p>这将控制生成器将在不接收响应的情况下发送到服务器的消息数。增加这个参数的值可以增加内存使用, 同时提高吞吐量, 但如果设置过高, 则会降低吞吐量, 因为批处理效率会降低。将其设置为1将保证消息将按发送时的顺序写入代理, 即使重试也是如此。</p>
<h2 id="参数测试"><a href="#参数测试" class="headerlink" title="参数测试"></a>参数测试</h2><h3 id="脚本准备"><a href="#脚本准备" class="headerlink" title="脚本准备"></a>脚本准备</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --bootstrap-server 10.153.1.78:9092,10.153.1.201:9092,10.153.0.208:9092 --create --partitions 3 --replication-factor 3 --topic ack-demo-topic</span><br><span class="line"></span><br><span class="line">bin/kafka-topics.sh --bootstrap-server 10.153.1.78:9092,10.153.1.201:9092,10.153.0.208:9092 --describe --topic ack-demo-topic</span><br><span class="line"></span><br><span class="line">bin/kafka-console-consumer.sh --bootstrap-server 10.153.1.78:9092,10.153.1.201:9092,10.153.0.208:9092 --topic ack-demo-topic</span><br></pre></td></tr></table></figure>
<h3 id="测试代码"><a href="#测试代码" class="headerlink" title="测试代码"></a>测试代码</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.zhang.producer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.KafkaProducer;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.ProducerRecord;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.Logger;</span><br><span class="line"><span class="keyword">import</span> org.slf4j.LoggerFactory;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"><span class="keyword">import</span> java.util.stream.IntStream;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Producer性能测试 使用 ack</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProducerPerTestWithAck</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">final</span> <span class="keyword">static</span> Logger LOGGER = LoggerFactory.getLogger(AsyncProducerDemo.class);</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Properties properties = initProps();</span><br><span class="line">        KafkaProducer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">        <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">        IntStream.range(<span class="number">0</span>, <span class="number">100000</span>).forEach(i -&gt; &#123;</span><br><span class="line">            ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(<span class="string">"ack-demo-topic"</span>, String.valueOf(i), <span class="string">"hello "</span> + i);</span><br><span class="line">            producer.send(record);</span><br><span class="line">        &#125;);</span><br><span class="line">        producer.flush();</span><br><span class="line">        producer.close();</span><br><span class="line">        <span class="keyword">long</span> current = System.currentTimeMillis();</span><br><span class="line">        LOGGER.info(<span class="string">"total const &#123;&#125; ms"</span>, (current - start));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 100000</span></span><br><span class="line"><span class="comment">     * ack=0 4737 ms</span></span><br><span class="line"><span class="comment">     * ack=1 6733 ms</span></span><br><span class="line"><span class="comment">     * ack=all 8144 ms</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * ack=0 snappy 5183 ms</span></span><br><span class="line"><span class="comment">     * ack=1 snappy 5665 ms</span></span><br><span class="line"><span class="comment">     * ack=all snappy 6325ms</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> Properties <span class="title">initProps</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">final</span> Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">"10.153.1.78:9092,10.153.1.201:9092,10.153.0.208:9092"</span>);</span><br><span class="line">        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, <span class="string">"org.apache.kafka.common.serialization.StringSerializer"</span>);</span><br><span class="line">        props.put(ProducerConfig.ACKS_CONFIG, <span class="string">"0"</span>);</span><br><span class="line">        props.put(ProducerConfig.COMPRESSION_TYPE_CONFIG,<span class="string">"snappy"</span>);</span><br><span class="line">        <span class="keyword">return</span> props;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/02/Kafka系列/2.Kafka安装和配置解析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/02/Kafka系列/2.Kafka安装和配置解析/" itemprop="url">Kafka安装和配置解析</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-02T11:37:00+08:00">
                2019-05-02
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/02/Kafka系列/2.Kafka安装和配置解析/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/02/Kafka系列/2.Kafka安装和配置解析/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/02/Kafka系列/2.Kafka安装和配置解析/" class="leancloud_visitors" data-flag-title="Kafka安装和配置解析">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="Kafka下载"><a href="#Kafka下载" class="headerlink" title="Kafka下载"></a>Kafka下载</h2><ul>
<li><p><a href="http://kafka.apache.org/" target="_blank" rel="noopener">http://kafka.apache.org/</a></p>
</li>
<li><p><a href="http://kafka.apache.org/documentation/" target="_blank" rel="noopener">http://kafka.apache.org/documentation/</a></p>
</li>
<li><p><a href="https://www.apache.org/dyn/closer.cgi?path=/kafka/2.2.0/kafka_2.12-2.2.0.tgz" target="_blank" rel="noopener">https://www.apache.org/dyn/closer.cgi?path=/kafka/2.2.0/kafka_2.12-2.2.0.tgz</a></p>
</li>
</ul>
<h2 id="Kafka安装"><a href="#Kafka安装" class="headerlink" title="Kafka安装"></a>Kafka安装</h2><h3 id="Kafka独立安装"><a href="#Kafka独立安装" class="headerlink" title="Kafka独立安装"></a>Kafka独立安装</h3><p><img src="/2019/05/02/Kafka系列/2.Kafka安装和配置解析/1.jpg" alt></p>
<h3 id="Kafka伪分布式安装"><a href="#Kafka伪分布式安装" class="headerlink" title="Kafka伪分布式安装"></a>Kafka伪分布式安装</h3><p><img src="/2019/05/02/Kafka系列/2.Kafka安装和配置解析/2.jpg" alt></p>
<h3 id="Kafka分布式安装"><a href="#Kafka分布式安装" class="headerlink" title="Kafka分布式安装"></a>Kafka分布式安装</h3><p><img src="/2019/05/02/Kafka系列/2.Kafka安装和配置解析/3.jpg" alt></p>
<blockquote>
<p>安装地址: <a href="http://kafka.apache.org/documentation/#quickstart" target="_blank" rel="noopener">http://kafka.apache.org/documentation/#quickstart</a></p>
</blockquote>
<h2 id="Broker基础配置解析"><a href="#Broker基础配置解析" class="headerlink" title="Broker基础配置解析"></a>Broker基础配置解析</h2><h3 id="1-broker-id"><a href="#1-broker-id" class="headerlink" title="1. broker.id"></a>1. broker.id</h3><p>此服务器的代理 id。如果取消设置, 将生成唯一的代理 id。为了避免zookeeper 生成的代理 id 和用户配置的代理 id 之间的冲突, 生成的代理 id 从 reserved.broker.max.id + 1 开始。</p>
<h3 id="2-port"><a href="#2-port" class="headerlink" title="2. port"></a>2. port</h3><p>DEPRECATED: 至少在未设置 <code>listeners</code>  时才使用。改为使用  <code>listeners</code> 。端口, 以侦听和接受连接。</p>
<h3 id="3-listeners"><a href="#3-listeners" class="headerlink" title="3. listeners"></a>3. listeners</h3><p>该参数指明 broker 监听客户端连接的地址列表，即为客户端要连接 broker 的入口地址列表，配置格式为 protocol1://hostname1:port1,protocol2://hostname2:port2，其中 protocol 代表协议类型，Kafka 当前支持的协议类型有 PLAINTEXT、SSL、SASL_SSL 等，如果未开启安全认证，则使用简单的 PLAINTEXT 即可。hostname 代表主机名，port 代表服务端口，此参数的默认值为 null。比如此参数配置为 PLAINTEXT://198.162.0.2:9092，如果有多个地址，则中间以逗号隔开。如果不指定主机名，则表示绑定默认网卡，注意有可能会绑定到127.0.0.1，这样无法对外提供服务，所以主机名最好不要为空；如果主机名是0.0.0.0，则表示绑定所有的网卡。</p>
<h3 id="4-zookeeper-connect"><a href="#4-zookeeper-connect" class="headerlink" title="4. zookeeper.connect"></a>4. zookeeper.connect</h3><p>该参数指明 broker 要连接的 ZooKeeper 集群的服务地址（包含端口号），没有默认值，且此参数为必填项。可以配置为 localhost:2181，如果 ZooKeeper 集群中有多个节点，则可以用逗号将每个节点隔开，类似于 localhost1:2181,localhost2:2181,localhost3:2181 这种格式。最佳的实践方式是再加一个 chroot 路径，这样既可以明确指明该 chroot 路径下的节点是为 Kafka 所用的，也可以实现多个 Kafka 集群复用一套 ZooKeeper 集群，这样可以节省更多的硬件资源。包含 chroot 路径的配置类似于 localhost1:2181,localhost2:2181,localhost3:2181/kafka 这种，如果不指定 chroot，那么默认使用 ZooKeeper 的根路径。</p>
<h3 id="5-log-dir和log-dirs"><a href="#5-log-dir和log-dirs" class="headerlink" title="5. log.dir和log.dirs"></a>5. log.dir和log.dirs</h3><p>Kafka 把所有的消息都保存在磁盘上，而这两个参数用来配置 Kafka 日志文件存放的根目录。一般情况下，log.dir 用来配置单个根目录，而 log.dirs 用来配置多个根目录（以逗号分隔），但是 Kafka 并没有对此做强制性限制，也就是说，log.dir 和 log.dirs 都可以用来配置单个或多个根目录。log.dirs 的优先级比 log.dir 高，但是如果没有配置 log.dirs，则会以 log.dir 配置为准。默认情况下只配置了 log.dir 参数，其默认值为 /tmp/kafka-logs。</p>
<h3 id="6-num-recovery-threads-per-data-dir"><a href="#6-num-recovery-threads-per-data-dir" class="headerlink" title="6.num.recovery.threads.per.data.dir"></a>6.num.recovery.threads.per.data.dir</h3><p>每个数据目录的线程数, 用于启动时的日志恢复和关机时的刷新</p>
<ul>
<li>Kafka 使用可配置的线程池来处理日志段。默认使用当前线程池</li>
<li>正常启动时, 打开每个分区的日志段</li>
<li>失败后启动时, 检查并截断每个分区的日志段</li>
<li>关机时, 要干净地关闭日志段</li>
<li>默认情况下, 每个日志目录仅使用一个线程。由于这些线程仅在启动和关闭期间使用, 因此为了并行化操作, 设置更多的线程是合理的。</li>
</ul>
<h3 id="7-auto-create-topics-enable"><a href="#7-auto-create-topics-enable" class="headerlink" title="7. auto.create.topics.enable"></a>7. auto.create.topics.enable</h3><p>在服务器上启用主题的自动创建</p>
<ul>
<li>Kafka 的默认配置指定在以下情况下, broker 自动创建主题</li>
<li>当生成器开始向主题写入消息时</li>
<li>当使用者开始阅读主题中的消息时</li>
<li>当任何客户端请求该主题的元数据时</li>
</ul>
<h3 id="8-num-partitions"><a href="#8-num-partitions" class="headerlink" title="8. num.partitions"></a>8. num.partitions</h3><p>每个主题的默认日志分区数</p>
<ul>
<li>num.partitions参数确定创建新主题的分区数, 主要是在启用自动主题创建时 (这是默认设置)。此参数默认为一个分区。请记住, 主题的分区数只能增加, 而不会减少。这意味着, 如果某个主题需要比 num.partitions少, 则需要注意手动创建该主题。</li>
</ul>
<h3 id="9-log-retention-ms"><a href="#9-log-retention-ms" class="headerlink" title="9. log.retention.ms"></a>9. log.retention.ms</h3><p>在删除日志文件之前保留日志文件的毫秒数 (以毫秒为单位), 如果未设置, 则使用 log.retention.minutes 的值</p>
<h3 id="10-log-retention-minutes"><a href="#10-log-retention-minutes" class="headerlink" title="10. log.retention.minutes"></a>10. log.retention.minutes</h3><p>删除日志文件之前保留日志文件的分钟数 (以分钟为中心), 仅次于 log.retention.ms 属性。如果未设置, 则使用 log.retention.hours 中的值</p>
<h3 id="11-log-retention-hours"><a href="#11-log-retention-hours" class="headerlink" title="11. log.retention.hours"></a>11. log.retention.hours</h3><p>删除日志文件之前保留日志文件的小时数 (以小时为个时间), 请按 log.retention.ms 属性。默认值168</p>
<h3 id="12-log-retention-bytes"><a href="#12-log-retention-bytes" class="headerlink" title="12. log.retention.bytes"></a>12. log.retention.bytes</h3><p>删除日志之前的最大大小</p>
<ul>
<li>使消息过期的另一种方法基于保留的消息的总字节数。此值是使用 log.retention.bytes 参数设置的, 并应用于每个分区。这意味着, 如果您有一个具有8个分区的主题, 并且 log.retention.bytes 设置为 1 GB, 则为该主题保留的数据量最多为 8 GB。</li>
</ul>
<h3 id="13-log-segment-bytes"><a href="#13-log-segment-bytes" class="headerlink" title="13. log.segment.bytes"></a>13. log.segment.bytes</h3><p>日志段上的日志保留设置, 而不是单个消息。当消息生成到Kafka broker, 它们将追加到分区的当前日志段中。日志段达到 log.segment.bytes 参数指定的大小 (默认值为 1 GB) 后, 日志段将关闭并打开一个新的日志段。日志段关闭后, 可以考虑将其过期。较小的日志段大小意味着必须更频繁地关闭和分配文件, 从而降低了磁盘写入的整体效率。如果主题的生成率较低, 则调整日志段的大小可能很重要。例如, 如果某个主题每天只收到100兆字节的消息, 并且 log.segment.bytes 设置为默认值, 则需要10天的时间来填充一个段。由于在日志段关闭之前消息无法过期, 因此如果 log.retention.ms 设置为 604800000 (1周), 则实际上将保留长达17天的消息, 直到关闭的日志段过期。这是因为, 一旦日志段关闭当前10天的消息, 该日志段必须保留 7天, 然后根据时间策略过期 (因为该段不能删除, 直到段中的最后一条消息可以过期)。</p>
<h3 id="14-log-segment-ms"><a href="#14-log-segment-ms" class="headerlink" title="14. log.segment.ms"></a>14. log.segment.ms</h3><p>控制日志段何时关闭的另一种方法是使用 log.segment.ms 参数, 该参数指定日志段关闭后的时间量。与 log.retention.bytes 和 log.retention.ms 参数一样, log.segment.bytes 和 log.segment.ms 不是相互排斥的属性。Kafka 将在达到大小限制或达到时间限制时关闭日志段, 以时间在前者为准。默认情况下, 没有 log.segment.ms 的设置, 这将导致仅按大小关闭日志段。</p>
<h3 id="15-message-max-bytes"><a href="#15-message-max-bytes" class="headerlink" title="15. message.max.bytes"></a>15. message.max.bytes</h3><p> Kafka broker限制message.max.bytes 参数配置的消息的最大大小, 该参数默认为1000012或 1 MB。试图发送大于此范围的消息的生成器将收到来自broker的错误, 并且该消息将不被接受。</p>
<h2 id="如何选择Broker数量？"><a href="#如何选择Broker数量？" class="headerlink" title="如何选择Broker数量？"></a>如何选择Broker数量？</h2><ul>
<li>要考虑的第一个因素是保留消息需要多少磁盘容量, 以及单个broker上有多少可用的存储空间。</li>
<li>要考虑的另一个因素是群集处理请求的能力。例如, 什么是网络接口的容量, 如果数据有多个使用者, 或者在数据的保留期内流量不一致, 它们是否可以处理客户端流量。</li>
</ul>
<h2 id="如何选择分区数量？"><a href="#如何选择分区数量？" class="headerlink" title="如何选择分区数量？"></a>如何选择分区数量？</h2><ul>
<li><p>您希望为该主题实现的吞吐量是多少？</p>
</li>
<li><p>如果要根据keys将消息发送到分区, 则以后添加分区可能非常具有挑战性, 因此, 请根据预期的未来使用情况 (而不是当前使用情况) 计算吞吐量。</p>
</li>
<li><p>考虑您将放置在每个broker上的分区数以及每个broker的可用磁盘空间和网络带宽。</p>
</li>
<li><p>避免高估, 因为每个分区使用broker的内存和其他资源, 并将增加领导人选举的时间。</p>
</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/05/01/Kafka系列/1.Kafka介绍/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="greenHard">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/timg.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="greenHard的个人主页">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/05/01/Kafka系列/1.Kafka介绍/" itemprop="url">Kafka介绍</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-05-01T10:23:00+08:00">
                2019-05-01
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Kafka学习笔记/" itemprop="url" rel="index">
                    <span itemprop="name">Kafka学习笔记</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/05/01/Kafka系列/1.Kafka介绍/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2019/05/01/Kafka系列/1.Kafka介绍/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2019/05/01/Kafka系列/1.Kafka介绍/" class="leancloud_visitors" data-flag-title="Kafka介绍">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Kafka 是由 LinkedIn 开发的一个分布式的消息系统，使用 Scala 编写，它以可水平扩展和高吞吐率而被广泛使用。目前越来越多的开源分布式处理系统如 Cloudera、Apache Storm、Spark 都支持与 Kafka 集成。</p>
<h2 id="Kafka简介"><a href="#Kafka简介" class="headerlink" title="Kafka简介"></a>Kafka简介</h2><p>Apache Kafka 是一个分布式流媒体平台</p>
<p>流媒体平台有三个关键功能:</p>
<ul>
<li>发布和订阅记录流，类似于消息队列或企业消息传递系统</li>
<li>以容错的持久方式存储记录流</li>
<li>记录发生时处理流</li>
</ul>
<p>Kafka通常用于两大类应用：</p>
<ul>
<li>构建可在系统或应用程序之间可靠获取数据的实时流数据管道</li>
<li>构建转换或响应数据流的实时流应用程序</li>
</ul>
<p>首先是几个概念：</p>
<ul>
<li>Kafka作为一个集群运行在一个或多个可跨多个数据中心的服务器上</li>
<li>Kafka集群以称为主题的类别存储记录流</li>
<li>每条记录由一个键、一个值和一个时间戳组成</li>
</ul>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/1.png" alt></p>
<p>在Kafka中，客户端和服务器之间的通信是通过简单，高性能，语言无关的<a href="https://kafka.apache.org/protocol.html" target="_blank" rel="noopener">TCP协议完成的</a>。此协议已版本化并保持与旧版本的向后兼容性。我们为Kafka提供Java客户端，但客户端有<a href="https://cwiki.apache.org/confluence/display/KAFKA/Clients" target="_blank" rel="noopener">多种语言版本</a>。</p>
<h2 id="Kafka厂商"><a href="#Kafka厂商" class="headerlink" title="Kafka厂商"></a>Kafka厂商</h2><ul>
<li>Apache Kafka  <a href="http://kafka.apache.org" target="_blank" rel="noopener">http://kafka.apache.org</a> (The development team at LinkedIn was led by Jay)</li>
<li>Hotworks Kafka <a href="https://hortonworks.com/apache/kafka/" target="_blank" rel="noopener">https://hortonworks.com/apache/kafka/</a></li>
<li>Confluent Io Kafka <a href="https://www.confluent.io" target="_blank" rel="noopener">https://www.confluent.io</a></li>
<li>Cloudera Kafka <a href="https://www.cloudera.com/" target="_blank" rel="noopener">https://www.cloudera.com/</a> (Doug Cutting )</li>
</ul>
<blockquote>
<p>Apache Kafka是一个社区分布式事件流平台，能够每天处理数万亿个事件。Kafka最初被设想为消息传递队列，它基于分布式提交日志的抽象。自2011年由LinkedIn创建并开源以来，Kafka已迅速从消息队列发展成为一个成熟的事件流平台。</p>
<p>由Apache Kafka的原始开发人员创建，Confluent通过Confluent Platform提供最完整的Kafka发行版。Confluent Platform通过其他社区和商业功能改进了Kafka，旨在大规模增强运营商和开发人员的生产流媒体体验。</p>
</blockquote>
<h2 id="Kafka特性"><a href="#Kafka特性" class="headerlink" title="Kafka特性"></a>Kafka特性</h2><ul>
<li>以时间复杂度为 O(1) 的方式提供消息持久化能力，即使对 TB 级以上数据也能保证常数时间复杂度的访问性能。</li>
</ul>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/2.png" alt></p>
<ul>
<li>高吞吐率。Kafka的设计是为了在商品硬件上工作并处理来自大量客户端的每秒数百 MBs 的读写。</li>
<li>分布式。支持 Kafka Server 间的消息分区，及分布式消费，同时保证每个 Partition 内的消息顺序传输。Kafka集群可以弹性和透明地增长, 而不会出现任何停机时间。</li>
<li>多客户端支持。Kafka系统支持来自不同平台 (如 Java、. net、PHP、ruby 和 Python) 的客户端的轻松集成。</li>
<li>同时支持离线数据处理和实时数据处理。</li>
</ul>
<h2 id="Kafka使用场景"><a href="#Kafka使用场景" class="headerlink" title="Kafka使用场景"></a>Kafka使用场景</h2><ul>
<li><p>消息</p>
<p><code>kafka</code>更好的替换传统的消息系统，消息系统被用于各种场景（解耦数据生产者，缓存未处理的消息等），与大多数消息系统比较，<code>kafka</code>有更好的吞吐量，内置分区，副本和故障转移，这有利于处理大规模的消息。</p>
<p>根据我们的经验，消息往往用于较低的吞吐量，但需要低的<code>端到端</code>延迟，并需要提供强大的耐用性的保证。</p>
</li>
<li><p>网站活动追踪</p>
<p><code>kafka</code>原本的使用场景：用户的活动追踪，网站的活动（网页游览，搜索或其他用户的操作信息）发布到不同的话题中心，这些消息可实时处理，实时监测，也可加载到<code>Hadoop</code>或离线处理数据仓库。</p>
<p>每个用户页面视图都会产生非常高的量。</p>
</li>
<li><p>指标</p>
<p><code>kafka</code>也常常用于监测数据。分布式应用程序生成的统计数据集中聚合。</p>
</li>
<li><p>日志聚合</p>
<p>许多人使用Kafka作为日志聚合解决方案的替代品。日志聚合通常从服务器中收集物理日志文件，并将它们放在中央位置（可能是文件服务器或<code>HDFS</code>）进行处理。<code>Kafka</code>抽象出文件的细节，并将日志或事件数据更清晰地抽象为消息流。这允许更低延迟的处理并更容易支持多个数据源和分布式数据消费。</p>
</li>
<li><p>流处理</p>
<p><code>kafka</code>中消息处理一般包含多个阶段。其中原始输入数据是从<code>kafka</code>主题消费的，然后汇总，丰富，或者以其他的方式处理转化为新主题，例如，一个推荐新闻文章，文章内容可能从“articles”主题获取；然后进一步处理内容，得到一个处理后的新内容，最后推荐给用户。这种处理是基于单个主题的实时数据流。从<code>0.10.0.0</code>开始，轻量，但功能强大的流处理，就可以这样进行数据处理了。</p>
<p>除了<code>Kafka Streams</code>，还有<code>Apache Storm</code>和<code>Apache Samza</code>可选择。</p>
</li>
<li><p>事件采集</p>
<p>事件采集是一种应用程序的设计风格，其中状态的变化根据时间的顺序记录下来，<code>kafka</code>支持这种非常大的存储日志数据的场景。</p>
</li>
<li><p>提交日志</p>
<p><code>kafka</code>可以作为一种分布式的外部日志，可帮助节点之间复制数据，并作为失败的节点来恢复数据重新同步，<code>kafka</code>的日志压缩功能很好的支持这种用法，这种用法类似于<code>Apacha BookKeeper</code>项目。</p>
<h2 id="Kafka名词解释"><a href="#Kafka名词解释" class="headerlink" title="Kafka名词解释"></a>Kafka名词解释</h2></li>
<li><p><code>Messages and Batches</code></p>
<ul>
<li><p><code>kafka</code>内的数据单位称为消息。</p>
</li>
<li><p>与行或记录类似的消息</p>
</li>
<li>就<code>kafka</code>而言, 消息只是一个字节数组</li>
<li>其中包含的数据对<code>kafka</code>没有具体的格式或含义</li>
<li>键也是一个字节数组, 和消息一样, 对<code>kafka</code>没有具体的含义。</li>
<li>为了提高效率, 信息成批写入<code>kafka</code>。</li>
<li>批处理只是消息的集合, 所有这些消息都被生成到同一主题和分区。</li>
<li>批处理通常也会被压缩, 从而以牺牲一定的处理能力为代价提供更高效的数据传输和存储。</li>
</ul>
</li>
<li><p><code>Schemas</code></p>
<ul>
<li>虽然消息是<code>kafka</code>本身的不透明阵列, 但建议在消息集内容上强加其他结构或模式, 以便易于理解。</li>
<li><code>Avro</code>、<code>Json</code>、<code>XML</code>等</li>
</ul>
</li>
<li><p><code>Topics and Partitions</code></p>
<ul>
<li><p><code>kafka</code>中的信息分为多个主题。</p>
</li>
<li><p>主题的最接近的类比是数据库表或文件系统中的文件夹。</p>
</li>
<li>主题还被分解为多个分区。</li>
<li>消息是以仅限附录的方式写入的, 并从开始到结束按顺序读取。</li>
<li>一个主题通常有多个分区, 不能保证整个主题的消息时间排序, 就在一个单一的范围内。</li>
</ul>
</li>
<li><p><code>Producers and Consumers</code></p>
<ul>
<li><code>kafka</code>客户是该系统的用户, 有两种基本类型: <code>producers</code> 和 <code>consumers</code>。</li>
<li><code>producers</code>创建新消息。在其他发布/订阅系统中, 这些系统可能被称为发布者或编写者。</li>
<li>通常, 一个消息会被生产到一个特定的主题。</li>
<li><code>consumer</code>读取消息。在其他发布/订阅系统中, 这些客户端可能被称为订阅者或读取器。</li>
<li><code>consumer</code>订阅一个或多个主题, 并按消息的生成顺序读取消息。</li>
<li><code>consumer</code>通过跟踪消息的<code>offset</code>来跟踪它已经使用的消息。</li>
<li>批处理只是消息的集合, 所有这些消息都被生成到同一主题和分区。</li>
</ul>
</li>
<li><p><code>Offset</code></p>
<ul>
<li><code>Offset</code>是另一个元数据位 (不断增加的整数值), <code>kafka</code>在生成每个消息时将其添加到该消息中。</li>
<li>给定分区中的每条消息都具有唯一的<code>Offset</code>。通过存储每个分区的最后消耗的消息的偏移量, 无论是在<code>zookeeper</code>中还是在<code>kafka</code>本身, 使用者可以停止并重新启动, 而不会失去它的位置。</li>
</ul>
</li>
<li><p><code>Consumer Group</code></p>
<ul>
<li><code>Consumer</code>作为<code>Consumer Group</code>的一部分工作,<code>Consumer Group</code>是一个或多个共同消费主题的<code>Consumer</code>。该组确保每个分区仅由一个成员使用。</li>
</ul>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/3.png" alt></p>
</li>
</ul>
<ul>
<li><code>Brokers and Clusters</code><ul>
<li>单个<code>kafka</code>服务器称为<code>Broker</code>。</li>
<li><code>Broker</code>接收来自<code>producer</code>的消息, 将<code>Offset</code>分配给它们, 并将消息提交到磁盘上的存储。</li>
<li><code>kafka Broker</code>的设计是作为集群的一部分运作。在一个<code>Brokers</code>集群中, 一个<code>Broker</code>还将充当集群<code>controller</code> (从集群的活动成员自动选择)</li>
</ul>
</li>
</ul>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/4.png" alt></p>
<ul>
<li><code>Multiple Clusters</code><ul>
<li>随着<code>Kafka</code>部署的增长, 拥有多个集群通常是有利的。有几个原因可以使用:<ul>
<li>数据类型的分离</li>
<li>安全要求的隔离</li>
<li>多个数据中心 (灾难恢复)</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/5.png" alt></p>
<h2 id="基本概念补充"><a href="#基本概念补充" class="headerlink" title="基本概念补充"></a>基本概念补充</h2><p>Kafka 为分区引入了多副本（Replica）机制，通过增加副本数量可以提升容灾能力。</p>
<p>同一分区的不同副本中保存的是相同的消息（在同一时刻，副本之间并非完全一样），副本之间是“一主多从”的关系，其中 leader 副本负责处理读写请求，follower 副本只负责与 leader 副本的消息同步。副本处于不同的 broker 中，当 leader 副本出现故障时，从 follower 副本中重新选举新的 leader 副本对外提供服务。Kafka 通过多副本机制实现了故障的自动转移，当 Kafka 集群中某个 broker 失效时仍然能保证服务可用。</p>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/6.png" alt></p>
<p>如上图所示，Kafka 集群中有4个 broker，某个主题中有3个分区，且副本因子（即副本个数）也为3，如此每个分区便有1个 leader 副本和2个 follower 副本。生产者和消费者只与 leader 副本进行交互，而 follower 副本只负责消息的同步，很多时候 follower 副本中的消息相对 leader 副本而言会有一定的滞后。</p>
<p>Kafka 消费端也具备一定的容灾能力。Consumer 使用拉（Pull）模式从服务端拉取消息，并且保存消费的具体位置，当消费者宕机后恢复上线时可以根据之前保存的消费位置重新拉取需要的消息进行消费，这样就不会造成消息丢失。</p>
<p>分区中的所有副本统称为 <strong>AR</strong>（Assigned Replicas）。所有与 leader 副本保持一定程度同步的副本（包括 leader 副本在内）组成<strong>ISR</strong>（In-Sync Replicas），ISR 集合是 AR 集合中的一个子集。消息会先发送到 leader 副本，然后 follower 副本才能从 leader 副本中拉取消息进行同步，同步期间内 follower 副本相对于 leader 副本而言会有一定程度的滞后。</p>
<p>前面所说的“一定程度的同步”是指可忍受的滞后范围，这个范围可以通过参数进行配置。与 leader 副本同步滞后过多的副本（不包括 leader 副本）组成 <strong>OSR</strong>（Out-of-Sync Replicas），由此可见，AR=ISR+OSR。在正常情况下，所有的 follower 副本都应该与 leader 副本保持一定程度的同步，即 AR=ISR，OSR 集合为空。</p>
<p>leader 副本负责维护和跟踪 ISR 集合中所有 follower 副本的滞后状态，当 follower 副本落后太多或失效时，leader 副本会把它从 ISR 集合中剔除。如果 OSR 集合中有 follower 副本“追上”了 leader 副本，那么 leader 副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当 leader 副本发生故障时，只有在 ISR 集合中的副本才有资格被选举为新的 leader，而在 OSR 集合中的副本则没有任何机会（不过这个原则也可以通过修改相应的参数配置来改变）。</p>
<p>ISR 与 HW 和 <strong>LEO</strong> 也有紧密的关系。<strong>HW</strong> 是 High Watermark 的缩写，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个 offset 之前的消息。</p>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/7.png" alt></p>
<p>如上图所示，它代表一个日志文件，这个日志文件中有9条消息，第一条消息的 offset（LogStartOffset）为0，最后一条消息的 offset 为8，offset 为9的消息用虚线框表示，代表下一条待写入的消息。日志文件的 HW 为6，表示消费者只能拉取到 offset 在0至5之间的消息，而 offset 为6的消息对消费者而言是不可见的。</p>
<p>LEO 是 Log End Offset 的缩写，它标识当前日志文件中下一条待写入消息的 offset，上图中 offset 为9的位置即为当前日志文件的 LEO，LEO 的大小相当于当前日志分区中最后一条消息的 offset 值加1。分区 ISR 集合中的每个副本都会维护自身的 LEO，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息。</p>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/8.png" alt></p>
<p>为了让读者更好地理解 ISR 集合，以及 HW 和 LEO 之间的关系，下面通过一个简单的示例来进行相关的说明。如上图所示，假设某个分区的 ISR 集合中有3个副本，即一个 leader 副本和2个 follower 副本，此时分区的 LEO 和 HW 都为3。消息3和消息4从生产者发出之后会被先存入 leader 副本，如下图所示。</p>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/9.png" alt></p>
<p>在消息写入 leader 副本之后，follower 副本会发送拉取请求来拉取消息3和消息4以进行消息同步。</p>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/10.png" alt></p>
<p>在同步过程中，不同的 follower 副本的同步效率也不尽相同。如上图所示，在某一时刻 follower1 完全跟上了 leader 副本而 follower2 只同步了消息3，如此 leader 副本的 LEO 为5，follower1 的 LEO 为5，follower2 的 LEO 为4，那么当前分区的 HW 取最小值4，此时消费者可以消费到 offset 为0至3之间的消息。</p>
<p>写入消息（情形4）如下图所示，所有的副本都成功写入了消息3和消息4，整个分区的 HW 和 LEO 都变为5，因此消费者可以消费到 offset 为4的消息了。</p>
<p><img src="/2019/05/01/Kafka系列/1.Kafka介绍/11.png" alt></p>
<p>由此可见，Kafka 的复制机制既不是完全的同步复制，也不是单纯的异步复制。事实上，同步复制要求所有能工作的 follower 副本都复制完，这条消息才会被确认为已成功提交，这种复制方式极大地影响了性能。而在异步复制方式下，follower 副本异步地从 leader 副本中复制数据，数据只要被 leader 副本写入就被认为已经成功提交。在这种情况下，如果 follower 副本都还没有复制完而落后于 leader 副本，突然 leader 副本宕机，则会造成数据丢失。Kafka 使用的这种 ISR 的方式则有效地权衡了数据可靠性和性能之间的关系。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>



          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/timg.png" alt="greenHard">
            
              <p class="site-author-name" itemprop="name">greenHard</p>
              <p class="site-description motion-element" itemprop="description">做个有梦想的咸鱼</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">27</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">12</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">greenHard</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://铜豌豆6号.disqus.com/count.js" async></script>
    

    

  




	





  














  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("OnzY68wYNuEreJmz6UKQ6WDi-gzGzoHsz", "1B6bWk99L50mdp8qQsktSQWK");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
